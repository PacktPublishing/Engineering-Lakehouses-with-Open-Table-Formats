{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – Install Python deps (inside the container)\n",
    "# Only needed if your image doesn't have these already.\n",
    "# In the jupyter/pyspark-notebook image, pyspark is present;\n",
    "# findspark is nice to have but optional.\n",
    "\n",
    "!pip install -q pyspark findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a00f3950c730:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache Hudi Chapter 4 Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xf6770c0f9d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41326)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 – Start SparkSession configured for Hudi\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "HUDI_VERSION = \"0.15.0\"\n",
    "SPARK_MAJOR = \"3.5\"   # Change to match your Spark (3.5, 3.4, 3.3, ...)\n",
    "\n",
    "hudi_bundle = f\"org.apache.hudi:hudi-spark{SPARK_MAJOR}-bundle_2.12:{HUDI_VERSION}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Apache Hudi Chapter 4 Demo\")\n",
    "    .config(\"spark.jars.packages\", hudi_bundle)\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|          1|      John|      Doe|john.doe@example.com| 150.75|   CA|\n",
      "|          2|      Jane|    Smith|jane.smith@exampl...|  950.0|   NY|\n",
      "|          3|       Tom|      Lee| tom.lee@example.com| 1200.0|   CA|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 – Simple example DataFrame (Spark + Hudi, COW)\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(customer_id=1, first_name=\"John\", last_name=\"Doe\",\n",
    "        email=\"john.doe@example.com\", charges=150.75, state=\"CA\"),\n",
    "    Row(customer_id=2, first_name=\"Jane\", last_name=\"Smith\",\n",
    "        email=\"jane.smith@example.com\", charges=950.00, state=\"NY\"),\n",
    "    Row(customer_id=3, first_name=\"Tom\", last_name=\"Lee\",\n",
    "        email=\"tom.lee@example.com\", charges=1200.00, state=\"CA\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 – Write a basic Copy‑On‑Write Hudi table\n",
    "base_path = \"/tmp/hudi_customers_cow\"\n",
    "table_name = \"hudi_customers_cow\"\n",
    "\n",
    "(\n",
    "    df.write\n",
    "    .format(\"hudi\")\n",
    "    .option(\"hoodie.datasource.write.table.type\", \"COPY_ON_WRITE\")\n",
    "    .option(\"hoodie.table.name\", table_name)\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"customer_id\")\n",
    "    .option(\"hoodie.datasource.write.partitionpath.field\", \"state\")\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"charges\")\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(base_path)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|  20251209052148125|20251209052148125...|                 3|              state=CA|1d9d92ae-0c92-481...|          3|       Tom|      Lee| tom.lee@example.com| 1200.0|   CA|\n",
      "|  20251209052148125|20251209052148125...|                 1|              state=CA|1d9d92ae-0c92-481...|          1|      John|      Doe|john.doe@example.com| 150.75|   CA|\n",
      "|  20251209052148125|20251209052148125...|                 2|              state=NY|274f850c-ff58-411...|          2|      Jane|    Smith|jane.smith@exampl...|  950.0|   NY|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 – Register table & basic snapshot query\n",
    "spark.sql(f\"DROP TABLE IF EXISTS customers_hudi_demo\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE customers_hudi_demo (\n",
    "  customer_id INT,\n",
    "  first_name  STRING,\n",
    "  last_name   STRING,\n",
    "  email       STRING,\n",
    "  charges     FLOAT,\n",
    "  state       STRING\n",
    ")\n",
    "USING hudi\n",
    "LOCATION '{base_path}'\n",
    "PARTITIONED BY (state)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM customers_hudi_demo\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 – DDL examples (CREATE, CTAS, ALTER, DROP)\n",
    "# CREATE TABLE (already done above, so we just re-show for completeness)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customers_hudi_demo (\n",
    "  customer_id INT,\n",
    "  first_name STRING,\n",
    "  last_name  STRING,\n",
    "  email      STRING,\n",
    "  charges    FLOAT,\n",
    "  state      STRING\n",
    ")\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS high_value_customers PURGE\n",
    "\"\"\")\n",
    "\n",
    "# CTAS example – high_value_customers\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS high_value_customers\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "AS\n",
    "SELECT\n",
    "  customer_id, first_name, last_name, state, charges\n",
    "FROM customers_hudi_demo\n",
    "WHERE charges > 1000\n",
    "\"\"\")\n",
    "\n",
    "# ALTER TABLE – add / rename / drop column\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE customers_hudi_demo\n",
    "ADD COLUMN phone_number STRING\n",
    "\"\"\")\n",
    "\n",
    "# This is not supported in Hudi\n",
    "# spark.sql(\"\"\"\n",
    "# ALTER TABLE customers_hudi_demo\n",
    "# DROP COLUMN phone_number\n",
    "# \"\"\")\n",
    "# Hudi/Spark SQL does NOT support ALTER TABLE ... RENAME COLUMN directly. Use CTAS workaround:\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE customers_hudi_demo_v2\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "AS\n",
    "SELECT customer_id, first_name, last_name, email, charges AS total_spent, phone_number, state\n",
    "FROM customers_hudi_demo\n",
    "\"\"\")\n",
    "\n",
    "# Optionally drop old table and rename new one to maintain continuity\n",
    "spark.sql(\"DROP TABLE customers_hudi_demo\")\n",
    "spark.sql(\"ALTER TABLE customers_hudi_demo_v2 RENAME TO customers_hudi_demo\")\n",
    "\n",
    "\n",
    "# DROP TABLE (catalog only)\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS high_value_customers PURGE\n",
    "\"\"\")\n",
    "\n",
    "# DROP TABLE and delete data – the PURGE syntax in the chapter\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS customers_hudi_demo PURGE\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|  20251209052153180|20251209052153180...|20251209052153180...|              state=NY|74d82a41-9353-440...|          2|      Jane|    Smith|jane.smith@exampl...|  250.0|   NY|\n",
      "|  20251209052152483|20251209052152483...|20251209052152483...|              state=CA|93064a8a-def8-4f4...|          1|      John|      Doe|john.doe@example.com| 150.75|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|  20251209052153180|20251209052153180...|20251209052153180...|              state=NY|74d82a41-9353-440...|          2|      Jane|    Smith|jane.smith@exampl...|  250.0|   NY|\n",
      "|  20251209052153916|20251209052153916...|20251209052153916...|              state=TX|73633b45-8653-43f...|          3|     Alice|    Brown|   alice@example.com|  300.0|   TX|\n",
      "|  20251209052153916|20251209052153916...|20251209052152483...|              state=CA|93064a8a-def8-4f4...|          1|      John|      Doe|john.new@example.com|  200.0|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `state` cannot be resolved. Did you mean one of the following? [`CA`, `Doe`, `John`, `220.0`, `customer_id`].; line 6 pos 6;\n'InsertIntoStatement Relation spark_catalog.default.customers_hudi_demo[customer_id#738,first_name#739,last_name#740,email#741,charges#742,state#743] parquet, [state=Some(CA)], true, false, false\n+- 'Project ['customer_id, 'first_name, 'last_name, 'email, 'charges, 'state]\n   +- 'Filter ('state = CA)\n      +- SubqueryAlias staging_updates\n         +- View (`staging_updates`, [customer_id#808,John#809,Doe#810,john.ca@example.com#811,220.0#812,CA#813])\n            +- Project [cast(customer_id#807 as int) AS customer_id#808, cast(John#814 as string) AS John#809, cast(Doe#815 as string) AS Doe#810, cast(john.ca@example.com#816 as string) AS john.ca@example.com#811, cast(220.0#817 as decimal(4,1)) AS 220.0#812, cast(CA#818 as string) AS CA#813]\n               +- Project [1 AS customer_id#807, John AS John#814, Doe AS Doe#815, john.ca@example.com AS john.ca@example.com#816, 220.0 AS 220.0#817, CA AS CA#818]\n                  +- OneRowRelation\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# INSERT OVERWRITE (static & dynamic)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Static overwrite of a single partition (CA)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124mCREATE OR REPLACE TEMP VIEW staging_updates AS\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124mSELECT 1 AS customer_id, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjohn.ca@example.com\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, 220.0, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;43mINSERT OVERWRITE customers_hudi_demo\u001b[39;49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;43mPARTITION (state = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;43mSELECT customer_id, first_name, last_name, email, charges, state\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;43mFROM staging_updates\u001b[39;49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;43mWHERE state = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Dynamic overwrite for multiple partitions\u001b[39;00m\n\u001b[1;32m     65\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124mINSERT OVERWRITE customers_hudi_demo\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124mSELECT customer_id, first_name, last_name, email, charges, state\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124mGROUP BY customer_id, first_name, last_name, email, charges, state\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `state` cannot be resolved. Did you mean one of the following? [`CA`, `Doe`, `John`, `220.0`, `customer_id`].; line 6 pos 6;\n'InsertIntoStatement Relation spark_catalog.default.customers_hudi_demo[customer_id#738,first_name#739,last_name#740,email#741,charges#742,state#743] parquet, [state=Some(CA)], true, false, false\n+- 'Project ['customer_id, 'first_name, 'last_name, 'email, 'charges, 'state]\n   +- 'Filter ('state = CA)\n      +- SubqueryAlias staging_updates\n         +- View (`staging_updates`, [customer_id#808,John#809,Doe#810,john.ca@example.com#811,220.0#812,CA#813])\n            +- Project [cast(customer_id#807 as int) AS customer_id#808, cast(John#814 as string) AS John#809, cast(Doe#815 as string) AS Doe#810, cast(john.ca@example.com#816 as string) AS john.ca@example.com#811, cast(220.0#817 as decimal(4,1)) AS 220.0#812, cast(CA#818 as string) AS CA#813]\n               +- Project [1 AS customer_id#807, John AS John#814, Doe AS Doe#815, john.ca@example.com AS john.ca@example.com#816, 220.0 AS 220.0#817, CA AS CA#818]\n                  +- OneRowRelation\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 – DML: INSERT, MERGE, INSERT OVERWRITE, DELETE, UPDATE\n",
    "# Recreate customers table for DML demo\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE customers_hudi_demo (\n",
    "  customer_id INT,\n",
    "  first_name  STRING,\n",
    "  last_name   STRING,\n",
    "  email       STRING,\n",
    "  charges     FLOAT,\n",
    "  state       STRING\n",
    ")\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO customers_hudi_demo\n",
    "VALUES (1, 'John', 'Doe', 'john.doe@example.com', 150.75, 'CA')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO customers_hudi_demo\n",
    "VALUES (2, 'Jane', 'Smith', 'jane.smith@example.com', 250.00, 'NY')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM customers_hudi_demo\").show()\n",
    "\n",
    "# MERGE INTO (upsert)\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW updates AS\n",
    "SELECT 1 AS customer_id, 'John' AS first_name, 'Doe' AS last_name,\n",
    "       'john.new@example.com' AS email, 200.00 AS charges, 'CA' AS state\n",
    "UNION ALL\n",
    "SELECT 3, 'Alice', 'Brown', 'alice@example.com', 300.00, 'TX'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO customers_hudi_demo AS target\n",
    "USING updates AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM customers_hudi_demo\").show()\n",
    "\n",
    "# INSERT OVERWRITE (static & dynamic)\n",
    "# Static overwrite of a single partition (CA)\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW staging_updates AS\n",
    "SELECT 1 AS customer_id, 'John', 'Doe', 'john.ca@example.com', 220.0, 'CA'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE customers_hudi_demo\n",
    "PARTITION (state = 'CA')\n",
    "SELECT customer_id, first_name, last_name, email, charges, state\n",
    "FROM staging_updates\n",
    "WHERE state = 'CA'\n",
    "\"\"\")\n",
    "\n",
    "# Dynamic overwrite for multiple partitions\n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE customers_hudi_demo\n",
    "SELECT customer_id, first_name, last_name, email, charges, state\n",
    "FROM staging_updates\n",
    "WHERE state IN ('CA')\n",
    "GROUP BY customer_id, first_name, last_name, email, charges, state\n",
    "\"\"\")\n",
    "\n",
    "# DELETE\n",
    "# Row-level delete\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM customers_hudi_demo\n",
    "WHERE customer_id = 1\n",
    "\"\"\")\n",
    "\n",
    "# Partition-level delete (metadata-only if partitioned by state)\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM customers_hudi_demo\n",
    "WHERE state = 'CA'\n",
    "\"\"\")\n",
    "\n",
    "# UPDATE\n",
    "spark.sql(\"\"\"\n",
    "UPDATE customers_hudi_demo\n",
    "SET charges = charges * 1.1\n",
    "WHERE state = 'NY'\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 – Read queries: snapshot, time travel, CDC, incremental\n",
    "# Snapshot queries with metadata + record index\n",
    "spark.sql(\"SET hoodie.enable.data.skipping=true\")\n",
    "spark.sql(\"SET hoodie.metadata.column.stats.enable=true\")\n",
    "spark.sql(\"SET hoodie.metadata.enable=true\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "WHERE charges > 1.0 AND charges < 1000.0\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"SET hoodie.metadata.record.index.enable=true\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "WHERE customer_id = 2\n",
    "\"\"\").show()\n",
    "\n",
    "# Time travel query (syntax template)\n",
    "time_travel_sql = \"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "TIMESTAMP AS OF '2025-01-01 00:00:00.000'\n",
    "WHERE charges > 100.0\n",
    "\"\"\"\n",
    "print(time_travel_sql)\n",
    "# spark.sql(time_travel_sql).show()  # Uncomment when you have a valid timestamp\n",
    "\n",
    "# CDC / incremental queries – use the hudi_table_changes TVF\n",
    "cdc_template = \"\"\"\n",
    "SELECT *\n",
    "FROM hudi_table_changes(\n",
    "  'customers_hudi_demo',\n",
    "  'cdc',\n",
    "  'earliest',  -- or starting commit time\n",
    "  NULL         -- optional end time\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "latest_state_template = \"\"\"\n",
    "SELECT *\n",
    "FROM hudi_table_changes(\n",
    "  'customers_hudi_demo',\n",
    "  'latest_state',\n",
    "  'earliest',  -- or starting commit time\n",
    "  NULL         -- optional end time\n",
    ")\n",
    "\"\"\"\n",
    "print(\"-- CDC template:\\n\", cdc_template)\n",
    "print(\"\\n-- Incremental latest_state template:\\n\", latest_state_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 – Common Hudi config “cheat sheet” from the chapter\n",
    "hudi_config_cheatsheet = {\n",
    "    # Schema evolution\n",
    "    \"hoodie.write.set.null.for.missing.columns\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "\n",
    "    # CDC logging mode\n",
    "    \"hoodie.table.cdc.supplemental.logging.mode\": \"op_key,op_old,op_new\",\n",
    "\n",
    "    # Compaction (MoR)\n",
    "    \"hoodie.compact.inline.max.delta.commits\": \"10\",\n",
    "    \"hoodie.datasource.compaction.async.enable\": \"true\",\n",
    "    \"hoodie.compact.inline\": \"true\",\n",
    "\n",
    "    # Cleaning / retention\n",
    "    \"hoodie.clean.automatic\": \"true\",\n",
    "    \"hoodie.cleaner.commits.retained\": \"10\",\n",
    "    \"hoodie.clean.async\": \"true\",\n",
    "\n",
    "    # File sizing\n",
    "    \"hoodie.parquet.small.file.limit\": \"104857600\",   # 100 MB\n",
    "    \"hoodie.parquet.max.file.size\": \"125829120\",      # 120 MB\n",
    "    \"hoodie.copyonwrite.record.size.estimate\": \"1024\",\n",
    "    \"hoodie.merge.small.file.group.candidates.limit\": \"5\",\n",
    "    \"hoodie.logfile.max.size\": \"1073741824\",          # 1 GB\n",
    "\n",
    "    # Clustering\n",
    "    \"hoodie.clustering.plan.strategy.small.file.limit\": \"134217728\",\n",
    "    \"hoodie.clustering.plan.strategy.target.file.max.bytes\": \"134217728\",\n",
    "\n",
    "    # Misc retention\n",
    "    \"hoodie.keep.max.commits\": \"20\",\n",
    "    \"hoodie.cleaner.fileversions.retained\": \"20\",\n",
    "}\n",
    "\n",
    "hudi_config_cheatsheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 – Stop Spark when done\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Flink + Hudi code snippets from the chapter\n",
    "\n",
    "The following commands are intended for a shell or Flink SQL, not Python:\n",
    "\n",
    "## 4.1 Environment setup\n",
    "```bash\n",
    "export FLINK_VERSION=1.17\n",
    "export HUDI_VERSION=0.15.0\n",
    "export HADOOP_HOME=/path/to/hadoop\n",
    "export HADOOP_CLASSPATH=\"$($HADOOP_HOME/bin/hadoop classpath)\"\n",
    "$FLINK_HOME/bin/start-cluster.sh\n",
    "wget \\\n",
    "  \"https://repo1.maven.org/maven2/org/apache/hudi/hudi-flink${FLINK_VERSION}-bundle/${HUDI_VERSION}/hudi-flink${FLINK_VERSION}-bundle-${HUDI_VERSION}.jar\" \\\n",
    "  -P \"$FLINK_HOME/lib/\"\n",
    "$FLINK_HOME/bin/sql-client.sh embedded \\\n",
    "  -j \"lib/hudi-flink${FLINK_VERSION}-bundle-${HUDI_VERSION}.jar\" \\\n",
    "  shell\n",
    "```\n",
    "\n",
    "## 4.2 Flink SQL Examples\n",
    "```sql\n",
    "CREATE CATALOG hudi_catalog\n",
    "WITH (\n",
    "  'type' = 'hudi',\n",
    "  'catalog.path' = 'file:///tmp/hudi_catalog',\n",
    "  'hive.conf.dir' = '/path/to/hive/conf',\n",
    "  'mode' = 'hms'\n",
    ");\n",
    "USE CATALOG hudi_catalog;\n",
    "CREATE DATABASE db;\n",
    "USE db;\n",
    "CREATE TABLE product_daily_price (\n",
    "  id   BIGINT PRIMARY KEY NOT ENFORCED,\n",
    "  name STRING,\n",
    "  price DOUBLE,\n",
    "  ts   BIGINT,\n",
    "  dt   STRING\n",
    ")\n",
    "PARTITIONED BY (dt)\n",
    "WITH (\n",
    "  'connector' = 'hudi',\n",
    "  'path' = 'file:///tmp/hudi_table',\n",
    "  'table.type' = 'MERGE_ON_READ',\n",
    "  'precombine.field' = 'ts',\n",
    "  'hoodie.cleaner.fileversions.retained' = '20',\n",
    "  'hoodie.keep.max.commits' = '20',\n",
    "  'hoodie.datasource.write.hive_style_partitioning' = 'true'\n",
    ");\n",
    "INSERT INTO product_daily_price\n",
    "SELECT 1, 'Lakehouse Book', 50, 1732256367, '2024-11-21';\n",
    "INSERT INTO product_daily_price + OPTIONS('write.operation' = 'upsert')\n",
    "SELECT 1, 'Lakehouse Book', 60, 1732256367, '2024-11-21';\n",
    "UPDATE product_daily_price\n",
    "SET price = price * 2, ts = 1732258867\n",
    "WHERE id = 1;\n",
    "DELETE FROM product_daily_price\n",
    "WHERE price < 50;\n",
    "INSERT INTO product_daily_price + OPTIONS('hoodie.keep.max.commits' = '10')\n",
    "SELECT 2, 'Another Book', 40, 1732256367, '2024-11-21';\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
