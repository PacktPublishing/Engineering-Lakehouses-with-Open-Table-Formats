{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Hudi Deep Dive (Chapter 4)\n",
    "\n",
    "End-to-end Hudi walkthrough for Spark: install dependencies, start Spark with the Hudi bundle, create a small customer dataset, write/read a Copy-On-Write table, run DDL/DML patterns, and validate results along the way. Each step is separated for clarity and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python deps inside the container if needed\n",
    "!pip install -q pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build a SparkSession configured for Hudi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUDI_VERSION = \"0.15.0\"\n",
    "SPARK_MAJOR = \"3.5\"  # Change to match your Spark (3.5, 3.4, 3.3, ...)\n",
    "\n",
    "hudi_bundle = f\"org.apache.hudi:hudi-spark{SPARK_MAJOR}-bundle_2.12:{HUDI_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Apache Hudi Chapter 4 Demo\")\n",
    "    .config(\"spark.jars.packages\", hudi_bundle)\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10d7f6de083e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache Hudi Chapter 4 Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x750a6010bf90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create a sample customer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sample_rows = [\n",
    "    Row(customer_id=1, first_name=\"John\", last_name=\"Doe\", email=\"john.doe@example.com\", charges=150.75, state=\"CA\"),\n",
    "    Row(customer_id=2, first_name=\"Jane\", last_name=\"Smith\", email=\"jane.smith@example.com\", charges=950.00, state=\"NY\"),\n",
    "    Row(customer_id=3, first_name=\"Tom\", last_name=\"Lee\", email=\"tom.lee@example.com\", charges=1200.00, state=\"CA\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(sample_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 3\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- charges: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|          1|      John|      Doe|john.doe@example.com| 150.75|   CA|\n",
      "|          2|      Jane|    Smith|jane.smith@exampl...|  950.0|   NY|\n",
      "|          3|       Tom|      Lee| tom.lee@example.com| 1200.0|   CA|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Row count:\", df.count())\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Write a Copy-On-Write Hudi table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/tmp/hudi_customers_cow\"\n",
    "table_name = \"customers_hudi_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.write\n",
    "    .format(\"hudi\")\n",
    "    .option(\"hoodie.datasource.write.table.type\", \"COPY_ON_WRITE\")\n",
    "    .option(\"hoodie.table.name\", table_name)\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"customer_id\")\n",
    "    .option(\"hoodie.datasource.write.partitionpath.field\", \"state\")\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"charges\")\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(base_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written rows: 3\n",
      "+-----------+-----+-------+\n",
      "|customer_id|state|charges|\n",
      "+-----------+-----+-------+\n",
      "|          1|   CA| 150.75|\n",
      "|          2|   NY|  950.0|\n",
      "|          3|   CA| 1200.0|\n",
      "+-----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "written_df = spark.read.format(\"hudi\").load(base_path)\n",
    "print(\"Written rows:\", written_df.count())\n",
    "written_df.select(\"customer_id\", \"state\", \"charges\").orderBy(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Register the table and run a snapshot query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS customers_hudi_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE customers_hudi_demo (\n",
    "  customer_id INT,\n",
    "  first_name  STRING,\n",
    "  last_name   STRING,\n",
    "  email       STRING,\n",
    "  charges     FLOAT,\n",
    "  state       STRING\n",
    ")\n",
    "USING hudi\n",
    "LOCATION '{base_path}'\n",
    "PARTITIONED BY (state)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "|  20251209073341872|20251209073341872...|                 1|              state=CA|e17666c0-25af-41a...|          1|      John|      Doe|john.doe@example.com| 150.75|   CA|\n",
      "|  20251209073341872|20251209073341872...|                 2|              state=NY|2d65233d-bfbe-45f...|          2|      Jane|    Smith|jane.smith@exampl...|  950.0|   NY|\n",
      "|  20251209073341872|20251209073341872...|                 3|              state=CA|e17666c0-25af-41a...|          3|       Tom|      Lee| tom.lee@example.com| 1200.0|   CA|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customers_hudi_demo ORDER BY customer_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) DDL examples (create, CTAS, alter, rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customers_hudi_demo (\n",
    "  customer_id INT,\n",
    "  first_name STRING,\n",
    "  last_name  STRING,\n",
    "  email      STRING,\n",
    "  charges    FLOAT,\n",
    "  state      STRING\n",
    ")\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS high_value_customers PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE high_value_customers\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "AS\n",
    "SELECT customer_id, first_name, last_name, state, charges\n",
    "FROM customers_hudi_demo\n",
    "WHERE charges > 1000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|charges|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------+-----+\n",
      "|  20251209073347349|20251209073347349...|20251209073347349...|              state=CA|4cb0eab4-b446-496...|          3|       Tom|      Lee| 1200.0|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM high_value_customers ORDER BY customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE customers_hudi_demo\n",
    "ADD COLUMN phone_number STRING\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE customers_hudi_demo_v2\n",
    "USING hudi\n",
    "PARTITIONED BY (state)\n",
    "AS\n",
    "SELECT customer_id, first_name, last_name, email, charges AS total_spent, phone_number, state\n",
    "FROM customers_hudi_demo\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE customers_hudi_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE customers_hudi_demo_v2 RENAME TO customers_hudi_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|_hoodie_commit_time    |string   |NULL   |\n",
      "|_hoodie_commit_seqno   |string   |NULL   |\n",
      "|_hoodie_record_key     |string   |NULL   |\n",
      "|_hoodie_partition_path |string   |NULL   |\n",
      "|_hoodie_file_name      |string   |NULL   |\n",
      "|customer_id            |bigint   |NULL   |\n",
      "|first_name             |string   |NULL   |\n",
      "|last_name              |string   |NULL   |\n",
      "|email                  |string   |NULL   |\n",
      "|total_spent            |double   |NULL   |\n",
      "|phone_number           |string   |NULL   |\n",
      "|state                  |string   |NULL   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|state                  |string   |NULL   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE customers_hudi_demo\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) DML operations (insert, merge, overwrite, delete, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hoodie.write.set.null.for.missing.columns=true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO customers_hudi_demo (customer_id, first_name, last_name, email, total_spent, phone_number, state)\n",
    "VALUES (1, 'John', 'Doe', 'john.doe@example.com', 150.75, NULL, 'CA')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO customers_hudi_demo (customer_id, first_name, last_name, email, total_spent, phone_number, state)\n",
    "VALUES (2, 'Jane', 'Smith', 'jane.smith@example.com', 250.00, NULL, 'NY')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "|  20251209073407411|20251209073407411...|20251209073407411...|              state=CA|aa7ffb9c-1e96-4bb...|          1|      John|      Doe|john.doe@example.com|     150.75|        NULL|   CA|\n",
      "|  20251209073426894|20251209073426894...|20251209073426894...|              state=CA|aa7ffb9c-1e96-4bb...|          1|      John|      Doe|john.doe@example.com|     150.75|        NULL|   CA|\n",
      "|  20251209073407411|20251209073407411...|20251209073407411...|              state=NY|f8b973e8-0b61-4b7...|          2|      Jane|    Smith|jane.smith@exampl...|      950.0|        NULL|   NY|\n",
      "|  20251209073439526|20251209073439526...|20251209073439526...|              state=NY|f8b973e8-0b61-4b7...|          2|      Jane|    Smith|jane.smith@exampl...|      250.0|        NULL|   NY|\n",
      "|  20251209073407411|20251209073407411...|20251209073407411...|              state=CA|aa7ffb9c-1e96-4bb...|          3|       Tom|      Lee| tom.lee@example.com|     1200.0|        NULL|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customers_hudi_demo ORDER BY customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW updates AS\n",
    "SELECT 1 AS customer_id, 'John' AS first_name, 'Doe' AS last_name,\n",
    "       'john.new@example.com' AS email, 200.00 AS total_spent, 'CA' AS state, CAST(NULL AS STRING) AS phone_number\n",
    "UNION ALL\n",
    "SELECT 3, 'Alice', 'Brown', 'alice@example.com', 300.00, 'TX', CAST(NULL AS STRING)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "MERGE INTO customers_hudi_demo AS target\n",
    "USING updates AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|               email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "|  20251209073455497|20251209073455497...|20251209073407411...|              state=CA|aa7ffb9c-1e96-4bb...|          1|      John|      Doe|john.new@example.com|      200.0|        NULL|   CA|\n",
      "|  20251209073455497|20251209073455497...|20251209073426894...|              state=CA|aa7ffb9c-1e96-4bb...|          1|      John|      Doe|john.new@example.com|      200.0|        NULL|   CA|\n",
      "|  20251209073407411|20251209073407411...|20251209073407411...|              state=NY|f8b973e8-0b61-4b7...|          2|      Jane|    Smith|jane.smith@exampl...|      950.0|        NULL|   NY|\n",
      "|  20251209073439526|20251209073439526...|20251209073439526...|              state=NY|f8b973e8-0b61-4b7...|          2|      Jane|    Smith|jane.smith@exampl...|      250.0|        NULL|   NY|\n",
      "|  20251209073455497|20251209073455497...|20251209073407411...|              state=CA|aa7ffb9c-1e96-4bb...|          3|     Alice|    Brown|   alice@example.com|      300.0|        NULL|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+--------------------+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customers_hudi_demo ORDER BY customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW staging_updates AS\n",
    "SELECT\n",
    "  1 AS customer_id,\n",
    "  'John' AS first_name,\n",
    "  'Doe' AS last_name,\n",
    "  'john.ca@example.com' AS email,\n",
    "  220.0 AS total_spent,\n",
    "  'CA' AS state,\n",
    "  CAST(NULL AS STRING) AS phone_number\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE customers_hudi_demo\n",
    "PARTITION (state = 'CA')\n",
    "SELECT customer_id, first_name, last_name, email, total_spent, phone_number\n",
    "FROM staging_updates\n",
    "WHERE state = 'CA'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE customers_hudi_demo\n",
    "SELECT customer_id, first_name, last_name, email, total_spent, phone_number, state\n",
    "FROM staging_updates\n",
    "GROUP BY customer_id, first_name, last_name, email, total_spent, phone_number, state\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------------------+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|customer_id|first_name|last_name|              email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------------------+-----------+------------+-----+\n",
      "|  20251209073536551|20251209073536551...|20251209073536551...|              state=CA|85169c22-6c8a-496...|          1|      John|      Doe|john.ca@example.com|      220.0|        NULL|   CA|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-----------+----------+---------+-------------------+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM customers_hudi_demo ORDER BY customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM customers_hudi_demo\n",
    "WHERE customer_id = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM customers_hudi_demo\n",
    "WHERE state = 'CA'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hudi often rejects `UPDATE` on meta columns; use MERGE-based update instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO customers_hudi_demo AS target\n",
    "USING (\n",
    "  SELECT customer_id, total_spent * 1.1 AS new_total\n",
    "  FROM customers_hudi_demo\n",
    "  WHERE state = 'NY'\n",
    ") AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.total_spent = source.new_total\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name|customer_id|first_name|last_name|email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM customers_hudi_demo ORDER BY customer_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Read patterns: snapshot, record index, time travel, CDC templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hoodie.enable.data.skipping=true\")\n",
    "spark.sql(\"SET hoodie.metadata.column.stats.enable=true\")\n",
    "spark.sql(\"SET hoodie.metadata.enable=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name|customer_id|first_name|last_name|email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "WHERE total_spent > 1.0 AND total_spent < 1000.0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name|customer_id|first_name|last_name|email|total_spent|phone_number|state|\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "+-------------------+--------------------+------------------+----------------------+-----------------+-----------+----------+---------+-----+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hoodie.metadata.record.index.enable=true\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "WHERE customer_id = 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-travel template: \n",
      "SELECT *\n",
      "FROM customers_hudi_demo\n",
      "TIMESTAMP AS OF '2025-01-01 00:00:00.000'\n",
      "WHERE total_spent > 100.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_travel_sql = \"\"\"\n",
    "SELECT *\n",
    "FROM customers_hudi_demo\n",
    "TIMESTAMP AS OF '2025-01-01 00:00:00.000'\n",
    "WHERE total_spent > 100.0\n",
    "\"\"\"\n",
    "print(\"Time-travel template:\", time_travel_sql)\n",
    "# spark.sql(time_travel_sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC template: \n",
      "SELECT *\n",
      "FROM hudi_table_changes(\n",
      "  'customers_hudi_demo',\n",
      "  'cdc',\n",
      "  'earliest',\n",
      "  NULL\n",
      ")\n",
      "\n",
      "Incremental latest_state template: \n",
      "SELECT *\n",
      "FROM hudi_table_changes(\n",
      "  'customers_hudi_demo',\n",
      "  'latest_state',\n",
      "  'earliest',\n",
      "  NULL\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdc_template = \"\"\"\n",
    "SELECT *\n",
    "FROM hudi_table_changes(\n",
    "  'customers_hudi_demo',\n",
    "  'cdc',\n",
    "  'earliest',\n",
    "  NULL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "latest_state_template = \"\"\"\n",
    "SELECT *\n",
    "FROM hudi_table_changes(\n",
    "  'customers_hudi_demo',\n",
    "  'latest_state',\n",
    "  'earliest',\n",
    "  NULL\n",
    ")\n",
    "\"\"\"\n",
    "print(\"CDC template:\", cdc_template)\n",
    "print(\"Incremental latest_state template:\", latest_state_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Common Hudi config cheat sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hoodie.write.set.null.for.missing.columns': 'true',\n",
       " 'hoodie.schema.on.read.enable': 'true',\n",
       " 'hoodie.table.cdc.supplemental.logging.mode': 'op_key,op_old,op_new',\n",
       " 'hoodie.compact.inline.max.delta.commits': '10',\n",
       " 'hoodie.datasource.compaction.async.enable': 'true',\n",
       " 'hoodie.compact.inline': 'true',\n",
       " 'hoodie.clean.automatic': 'true',\n",
       " 'hoodie.cleaner.commits.retained': '10',\n",
       " 'hoodie.clean.async': 'true',\n",
       " 'hoodie.parquet.small.file.limit': '104857600',\n",
       " 'hoodie.parquet.max.file.size': '125829120',\n",
       " 'hoodie.copyonwrite.record.size.estimate': '1024',\n",
       " 'hoodie.merge.small.file.group.candidates.limit': '5',\n",
       " 'hoodie.logfile.max.size': '1073741824',\n",
       " 'hoodie.clustering.plan.strategy.small.file.limit': '134217728',\n",
       " 'hoodie.clustering.plan.strategy.target.file.max.bytes': '134217728',\n",
       " 'hoodie.keep.max.commits': '20',\n",
       " 'hoodie.cleaner.fileversions.retained': '20'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hudi_config_cheatsheet = {\n",
    "    \"hoodie.write.set.null.for.missing.columns\": \"true\",\n",
    "    \"hoodie.schema.on.read.enable\": \"true\",\n",
    "    \"hoodie.table.cdc.supplemental.logging.mode\": \"op_key,op_old,op_new\",\n",
    "    \"hoodie.compact.inline.max.delta.commits\": \"10\",\n",
    "    \"hoodie.datasource.compaction.async.enable\": \"true\",\n",
    "    \"hoodie.compact.inline\": \"true\",\n",
    "    \"hoodie.clean.automatic\": \"true\",\n",
    "    \"hoodie.cleaner.commits.retained\": \"10\",\n",
    "    \"hoodie.clean.async\": \"true\",\n",
    "    \"hoodie.parquet.small.file.limit\": \"104857600\",\n",
    "    \"hoodie.parquet.max.file.size\": \"125829120\",\n",
    "    \"hoodie.copyonwrite.record.size.estimate\": \"1024\",\n",
    "    \"hoodie.merge.small.file.group.candidates.limit\": \"5\",\n",
    "    \"hoodie.logfile.max.size\": \"1073741824\",\n",
    "    \"hoodie.clustering.plan.strategy.small.file.limit\": \"134217728\",\n",
    "    \"hoodie.clustering.plan.strategy.target.file.max.bytes\": \"134217728\",\n",
    "    \"hoodie.keep.max.commits\": \"20\",\n",
    "    \"hoodie.cleaner.fileversions.retained\": \"20\",\n",
    "}\n",
    "\n",
    "hudi_config_cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Shut down Spark when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Flink + Hudi code snippets (for shell / Flink SQL)\n",
    "\n",
    "The commands below run in a shell or Flink SQL CLI, not in Python.\n",
    "\n",
    "## 4.1 Environment setup\n",
    "```bash\n",
    "export FLINK_VERSION=1.17\n",
    "export HUDI_VERSION=0.15.0\n",
    "export HADOOP_HOME=/path/to/hadoop\n",
    "export HADOOP_CLASSPATH=\"$($HADOOP_HOME/bin/hadoop classpath)\"\n",
    "$FLINK_HOME/bin/start-cluster.sh\n",
    "wget   \"https://repo1.maven.org/maven2/org/apache/hudi/hudi-flink${FLINK_VERSION}-bundle/${HUDI_VERSION}/hudi-flink${FLINK_VERSION}-bundle-${HUDI_VERSION}.jar\"   -P \"$FLINK_HOME/lib/\"\n",
    "$FLINK_HOME/bin/sql-client.sh embedded   -j \"lib/hudi-flink${FLINK_VERSION}-bundle-${HUDI_VERSION}.jar\"   shell\n",
    "```\n",
    "\n",
    "## 4.2 Flink SQL Examples\n",
    "```sql\n",
    "CREATE CATALOG hudi_catalog\n",
    "WITH (\n",
    "  'type' = 'hudi',\n",
    "  'catalog.path' = 'file:///tmp/hudi_catalog',\n",
    "  'hive.conf.dir' = '/path/to/hive/conf',\n",
    "  'mode' = 'hms'\n",
    ");\n",
    "USE CATALOG hudi_catalog;\n",
    "CREATE DATABASE db;\n",
    "USE db;\n",
    "CREATE TABLE product_daily_price (\n",
    "  id   BIGINT PRIMARY KEY NOT ENFORCED,\n",
    "  name STRING,\n",
    "  price DOUBLE,\n",
    "  ts   BIGINT,\n",
    "  dt   STRING\n",
    ")\n",
    "PARTITIONED BY (dt)\n",
    "WITH (\n",
    "  'connector' = 'hudi',\n",
    "  'path' = 'file:///tmp/hudi_table',\n",
    "  'table.type' = 'MERGE_ON_READ',\n",
    "  'precombine.field' = 'ts',\n",
    "  'hoodie.cleaner.fileversions.retained' = '20',\n",
    "  'hoodie.keep.max.commits' = '20',\n",
    "  'hoodie.datasource.write.hive_style_partitioning' = 'true'\n",
    ");\n",
    "INSERT INTO product_daily_price\n",
    "SELECT 1, 'Lakehouse Book', 50, 1732256367, '2024-11-21';\n",
    "INSERT INTO product_daily_price + OPTIONS('write.operation' = 'upsert')\n",
    "SELECT 1, 'Lakehouse Book', 60, 1732256367, '2024-11-21';\n",
    "UPDATE product_daily_price\n",
    "SET price = price * 2, ts = 1732258867\n",
    "WHERE id = 1;\n",
    "DELETE FROM product_daily_price\n",
    "WHERE price < 50;\n",
    "INSERT INTO product_daily_price + OPTIONS('hoodie.keep.max.commits' = '10')\n",
    "SELECT 2, 'Another Book', 40, 1732256367, '2024-11-21';\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
