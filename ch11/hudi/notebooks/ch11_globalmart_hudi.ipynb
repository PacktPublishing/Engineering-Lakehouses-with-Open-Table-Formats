{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0e0233",
   "metadata": {},
   "source": [
    "# Chapter 11 — GlobalMart: Apache Hudi CDC Pipeline (Hudi section only)\n",
    "\n",
    "This notebook contains the **ready-to-run code** for the GlobalMart Hudi CDC pipeline described in Chapter 11.\n",
    "\n",
    "It focuses on:\n",
    "- Parsing **Debezium** change events (`before`/`after`/`op`)\n",
    "- Transforming events into a **Hudi-friendly** shape\n",
    "- Applying **upserts + deletes** using Hudi’s supported delete marker: **`_hoodie_is_deleted`**\n",
    "- Writing to a **Merge-on-Read** Hudi table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087892e6",
   "metadata": {},
   "source": [
    "## 0) Prerequisites (runtime)\n",
    "\n",
    "You need a Spark runtime that includes the Hudi Spark bundle compatible with your Spark version.\n",
    "\n",
    "Typical options:\n",
    "- Run this notebook **inside the chapter’s Docker environment** (recommended).\n",
    "- Or configure Spark with the Hudi bundle (e.g., `--packages org.apache.hudi:hudi-spark3.4-bundle_2.12:<HUDI_VERSION>`), depending on your Spark/Hudi versions.\n",
    "\n",
    "> Note: The code below is written to be **copy/paste friendly** into your Chapter 11 repo under `ch11/` and used as the canonical reference for the Hudi section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d33bf",
   "metadata": {},
   "source": [
    "### Choose your path\n",
    "- **Quick demo (no Kafka needed):** run the seed/batch cells below, then snapshot/changelog.\n",
    "- **Full CDC (Debezium→Kafka→Spark):** run the streaming cell after starting the stack, optionally pump MySQL changes via the helper script, then rerun snapshot/changelog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3016152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports used throughout the notebook ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, current_timestamp, lit, when, coalesce\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b5653",
   "metadata": {},
   "source": [
    "## 1) Create / reuse SparkSession\n",
    "\n",
    "If you are running inside a pre-built image, you may already have a SparkSession (`spark`) available.\n",
    "If not, create one here. Adjust configs to match your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94875f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession only if not already present (common in managed notebook runtimes)\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "            .appName(\"Ch11-GlobalMart-Hudi-CDC\")\n",
    "            # If your runtime requires Hudi SQL extensions, uncomment and tune these:\n",
    "            # .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "            # .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c05247",
   "metadata": {},
   "source": [
    "## 2) Define the Debezium CDC schema + Hudi table schema/options\n",
    "\n",
    "This section addresses the review comment:\n",
    "- **`cdc_schema` must be explicitly defined** (otherwise `from_json(...)` fails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13659075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Debezium envelope schema (simplified) ---\n",
    "# Debezium emits an envelope with before/after/op/ts_ms.\n",
    "# For deletes: `after` is null and `before` contains the deleted row.\n",
    "\n",
    "row_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"updated_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "cdc_schema = StructType([\n",
    "    StructField(\"before\", row_schema, True),\n",
    "    StructField(\"after\", row_schema, True),\n",
    "    StructField(\"op\", StringType(), True),\n",
    "    StructField(\"ts_ms\", LongType(), True)\n",
    "])\n",
    "\n",
    "# --- Hudi table options ---\n",
    "# This matches the Chapter 11 narrative: MERGE_ON_READ + precombine field + partitioning.\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": \"products_hudi\",\n",
    "    # Some Hudi/Spark configurations also expect this explicit table-name key:\n",
    "    \"hoodie.datasource.write.table.name\": \"products_hudi\",\n",
    "\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"product_id\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"updated_at\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"category\",\n",
    "    \"hoodie.datasource.write.table.type\": \"MERGE_ON_READ\",\n",
    "\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\": \"4\",\n",
    "    \"hoodie.upsert.shuffle.parallelism\": \"4\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "\n",
    "    \"hoodie.compact.async.enable\": \"true\",\n",
    "    \"hoodie.clustering.async.enabled\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcf772",
   "metadata": {},
   "source": [
    "## Quick demo (no Kafka required)\n",
    "Run these to populate Hudi locally and validate immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0efe6",
   "metadata": {},
   "source": [
    "### Seed if empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27615380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hudi table already has data; skipping seed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "# Ensure paths are defined even if env vars missing\n",
    "HUDI_BASE_PATH = globals().get('HUDI_BASE_PATH', '/data/hudi/products_hudi')\n",
    "CHECKPOINT_PATH = globals().get('CHECKPOINT_PATH', '/data/checkpoints/products')\n",
    "\n",
    "\n",
    "def seed_demo_if_empty():\n",
    "    try:\n",
    "        existing = spark.read.format('hudi').load(HUDI_BASE_PATH)\n",
    "        if existing.limit(1).count() > 0:\n",
    "            print('Hudi table already has data; skipping seed.')\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    seed_rows = [\n",
    "        (301, 'Seed Widget', 'Tools', 40.0, 'seed v1', False),\n",
    "        (302, 'Seed Gadget', 'Electronics', 99.0, 'seed v1', False),\n",
    "        (301, 'Seed Widget', 'Tools', None, 'tombstone', True),  # delete to show tombstone\n",
    "    ]\n",
    "    seed_df = spark.createDataFrame(seed_rows, ['product_id','name','category','price','description','_hoodie_is_deleted'])\n",
    "    seed_df = seed_df.withColumn('updated_at', current_timestamp())\n",
    "    (seed_df\n",
    "        .write\n",
    "        .format('hudi')\n",
    "        .options(**hudi_options)\n",
    "        .mode('append')\n",
    "        .save(HUDI_BASE_PATH)\n",
    "    )\n",
    "    print('Seeded demo rows into', HUDI_BASE_PATH)\n",
    "\n",
    "seed_demo_if_empty()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1b5ba",
   "metadata": {},
   "source": [
    "### Optional batch demo (insert→update→delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ed0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote demo batch rows to Hudi at /data/hudi/products_hudi\n"
     ]
    }
   ],
   "source": [
    "# Write a small batch of CDC-like events directly to Hudi (for demo visibility)\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "batch_events = [\n",
    "    (201, 'Batch Widget', 'Tools', 30.0, 'batch v1', False),\n",
    "    (201, 'Batch Widget', 'Tools', 32.5, 'batch v2', False),\n",
    "    (201, 'Batch Widget', 'Tools', None, 'batch tombstone', True),  # delete\n",
    "]\n",
    "batch_df = spark.createDataFrame(batch_events, ['product_id','name','category','price','description','_hoodie_is_deleted'])\n",
    "batch_df = batch_df.withColumn('updated_at', current_timestamp())\n",
    "(batch_df\n",
    "    .write\n",
    "    .format('hudi')\n",
    "    .options(**hudi_options)\n",
    "    .mode('append')\n",
    "    .save(HUDI_BASE_PATH)\n",
    ")\n",
    "print('Wrote demo batch rows to Hudi at', HUDI_BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624403c1",
   "metadata": {},
   "source": [
    "### Validate snapshot (latest state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54118b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+----------+-----------+-----------+-----+-----------+------------------+--------------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                        |product_id|name       |category   |price|description|_hoodie_is_deleted|updated_at                |\n",
      "+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+----------+-----------+-----------+-----+-----------+------------------+--------------------------+\n",
      "|20251212194018880  |20251212194018880_1_0|302               |Electronics           |49249cc2-d50b-4a83-af7f-11c89b9af9ac-0_1-54-125_20251212194018880.parquet|302       |Seed Gadget|Electronics|99.0 |seed v1    |false             |2025-12-12 19:40:19.050833|\n",
      "+-------------------+---------------------+------------------+----------------------+-------------------------------------------------------------------------+----------+-----------+-----------+-----+-----------+------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Snapshot read: latest state (delete leaves table empty)\n",
    "snapshot_df = (\n",
    "    spark.read.format('hudi')\n",
    "    .load(HUDI_BASE_PATH)\n",
    "    .orderBy('product_id', '_hoodie_commit_time')\n",
    ")\n",
    "snapshot_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22400f0a",
   "metadata": {},
   "source": [
    "### Validate changelog (history with deletes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ca3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|product_id|name       |category   |price|_hoodie_is_deleted|_hoodie_commit_time|\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|302       |Seed Gadget|Electronics|99.0 |false             |20251212194018880  |\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Changelog read: see every upsert/delete that arrived (including _hoodie_is_deleted)\n",
    "changelog_df = (\n",
    "    spark.read.format('hudi')\n",
    "    .option('hoodie.datasource.query.type', 'incremental')\n",
    "    .option('hoodie.datasource.read.begin.instanttime', '000')\n",
    "    .load(HUDI_BASE_PATH)\n",
    "    .orderBy('_hoodie_commit_time', 'product_id')\n",
    ")\n",
    "changelog_df.select('product_id','name','category','price','_hoodie_is_deleted','_hoodie_commit_time').show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703be67",
   "metadata": {},
   "source": [
    "## Full CDC path (Debezium → Kafka → Spark Streaming → Hudi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb5b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Kafka source configuration (edit for your environment) ---\n",
    "import os\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.environ.get(\"KAFKA_BOOTSTRAP\", \"kafka:29092\")\n",
    "KAFKA_TOPIC = os.environ.get(\"KAFKA_TOPIC\", \"globalmart.retail.products\")\n",
    "\n",
    "HUDI_BASE_PATH = os.environ.get(\"HUDI_BASE_PATH\", \"/data/hudi/products_hudi\")\n",
    "CHECKPOINT_PATH = os.environ.get(\"CHECKPOINT_PATH\", \"/data/checkpoints/products\")\n",
    "\n",
    "# Read CDC events from Kafka\n",
    "cdc_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "        .option(\"subscribe\", KAFKA_TOPIC)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .load()\n",
    ")\n",
    "\n",
    "# Parse the CDC event payload\n",
    "parsed_stream = (\n",
    "    cdc_stream\n",
    "        .select(from_json(col(\"value\").cast(\"string\"), cdc_schema).alias(\"data\"))\n",
    "        .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# Transform to a Hudi-ready shape.\n",
    "# IMPORTANT: Hudi interprets deletes via the `_hoodie_is_deleted` flag (true for deletes, false/null otherwise).\n",
    "transformed_stream = (\n",
    "    parsed_stream\n",
    "        .select(\n",
    "            # record key must be present even for deletes (where after is null)\n",
    "            coalesce(col(\"after.product_id\"), col(\"before.product_id\")).alias(\"product_id\"),\n",
    "\n",
    "            # keep latest state for upserts; for deletes, these may be null (acceptable)\n",
    "            coalesce(col(\"after.name\"), col(\"before.name\")).alias(\"name\"),\n",
    "            coalesce(col(\"after.category\"), col(\"before.category\")).alias(\"category\"),\n",
    "            coalesce(col(\"after.price\"), col(\"before.price\")).alias(\"price\"),\n",
    "            coalesce(col(\"after.description\"), col(\"before.description\")).alias(\"description\"),\n",
    "\n",
    "            current_timestamp().alias(\"updated_at\"),\n",
    "            when(col(\"op\") == \"d\", lit(True)).otherwise(lit(False)).alias(\"_hoodie_is_deleted\")\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71149ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped streaming query after 60s; rerun this cell to ingest more CDC.\n"
     ]
    }
   ],
   "source": [
    "# Start the streaming write to Hudi (runs ~60s then stops)\n",
    "query = (\n",
    "    transformed_stream\n",
    "        .writeStream\n",
    "        .format(\"hudi\")\n",
    "        .options(**hudi_options)\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"path\", HUDI_BASE_PATH)\n",
    "        .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "        .trigger(processingTime=\"30 seconds\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "# Let it run for ~60 seconds to ingest CDC events, then stop.\n",
    "query.awaitTermination(60)\n",
    "query.stop()\n",
    "print(\"Stopped streaming query after 60s; rerun this cell to ingest more CDC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3888d7",
   "metadata": {},
   "source": [
    "## 6) Notes for Chapter 11 readers\n",
    "\n",
    "- If you are using Debezium with schema+payload enabled, you may need to unwrap the envelope before applying `from_json(...)`.\n",
    "- For real pipelines, you may also want:\n",
    "  - a dead-letter queue for malformed CDC events\n",
    "  - metrics on lag and commit throughput\n",
    "  - explicit Hudi write tuning (parallelism, compaction cadence, clustering strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4eaecd",
   "metadata": {},
   "source": [
    "### Validate CDC output (summary)\n",
    "Run after snapshot/changelog to see counts and latest commit times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed263fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot rows: 1\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|product_id|name       |category   |price|_hoodie_is_deleted|_hoodie_commit_time|\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|302       |Seed Gadget|Electronics|99.0 |false             |20251212194018880  |\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "\n",
      "Changelog rows: 1\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|product_id|name       |category   |price|_hoodie_is_deleted|_hoodie_commit_time|\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "|302       |Seed Gadget|Electronics|99.0 |false             |20251212194018880  |\n",
      "+----------+-----------+-----------+-----+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick CDC validation summary\n",
    "HUDI_BASE_PATH = globals().get('HUDI_BASE_PATH', '/data/hudi/products_hudi')\n",
    "snap = spark.read.format('hudi').load(HUDI_BASE_PATH)\n",
    "print('Snapshot rows:', snap.count())\n",
    "snap.select('product_id','name','category','price','_hoodie_is_deleted','_hoodie_commit_time').orderBy('_hoodie_commit_time').show(20, truncate=False)\n",
    "\n",
    "chg = (spark.read.format('hudi')\n",
    "    .option('hoodie.datasource.query.type','incremental')\n",
    "    .option('hoodie.datasource.read.begin.instanttime','000')\n",
    "    .load(HUDI_BASE_PATH))\n",
    "print('Changelog rows:', chg.count())\n",
    "chg.select('product_id','name','category','price','_hoodie_is_deleted','_hoodie_commit_time').orderBy('_hoodie_commit_time').show(50, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
