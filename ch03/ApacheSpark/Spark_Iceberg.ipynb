{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfec4be5",
   "metadata": {},
   "source": [
    "# Configure Iceberg + Apache Spark + Hadoop Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834a6327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_REGION=us-east-2\n",
      "env: AWS_ACCESS_KEY_ID=key\n",
      "env: AWS_SECRET_ACCESS_KEY=secret\n"
     ]
    }
   ],
   "source": [
    "# Define the AWS env variables if you are using AWS Auth:\n",
    "%env AWS_REGION= us-east-2\n",
    "%env AWS_ACCESS_KEY_ID= key\n",
    "%env AWS_SECRET_ACCESS_KEY= secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98167685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/docker/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/docker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/docker/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-aadf4fc2-2a29-4947-9c1b-f45b3d576f79;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.4.3 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      ":: resolution report :: resolve 6640ms :: artifacts dl 269ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.4.3 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-aadf4fc2-2a29-4947-9c1b-f45b3d576f79\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/190ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 00:15:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 00:15:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.hdfs_catalog', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.hdfs_catalog.type', 'hadoop')\n",
    "        .set('spark.sql.catalog.hdfs_catalog.warehouse', 's3a://diplakehouse/test_iceberg_book/')\n",
    "        .set('spark.sql.catalog.hdfs_catalog.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.hadoop.fs.s3a.access.key', 'key')\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', 'secret')\n",
    ")\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d1b15",
   "metadata": {},
   "source": [
    "# Create Table Customers using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346cc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/08 22:41:31 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hdfs_catalog.customers (\n",
    "        customer_id INT,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        charges FLOAT,\n",
    "        state STRING)\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (state)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8208b04",
   "metadata": {},
   "source": [
    "# Create Table Customers using Spark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8ea422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType \n",
    "\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "schema = StructType([ \n",
    "\n",
    "    StructField(\"customer_id\", IntegerType(), True), \n",
    "\n",
    "    StructField(\"first_name\", StringType(), True), \n",
    "\n",
    "    StructField(\"last_name\", StringType(), True), \n",
    "\n",
    "    StructField(\"email\", StringType(), True), \n",
    "\n",
    "    StructField(\"charges\", FloatType(), True), \n",
    "\n",
    "    StructField(\"state\", StringType(), True) \n",
    "\n",
    "]) \n",
    "\n",
    "df = spark.createDataFrame([], schema) \n",
    "\n",
    "df.writeTo(\"hdfs_catalog.customers_new\").partitionedBy(col(\"state\")).create() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd80b81",
   "metadata": {},
   "source": [
    "# Create Table As (CTAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50602fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "    CREATE TABLE hdfs_catalog.high_value_customers \n",
    "    USING iceberg\n",
    "    PARTITIONED BY (state)\n",
    "    AS SELECT customer_id, first_name, last_name, state, charges\n",
    "    FROM hdfs_catalog.customers \n",
    "    WHERE charges > 100\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2814af",
   "metadata": {},
   "source": [
    "# Create Table As (CTAS) using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctas = spark.read.table(\"hdfs_catalog.customers\") \n",
    "\n",
    "df_ctas.filter(df_ctas.charges > 1000) \\ \n",
    "       .select(\"customer_id\", \"first_name\", \"last_name\", \"state\", \"charges\") \\ \n",
    "       .writeTo(\"hdfs_catalog.high_value_customers\") \\ \n",
    "       .partitionedBy(\"state\") \\ \n",
    "       .create() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33147267",
   "metadata": {},
   "source": [
    "# Drop a table without deleting its data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9dbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hdfs_catalog.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606c681",
   "metadata": {},
   "source": [
    "# Drop a table and delete its data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2acc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS hdfs_catalog.customers PURGE\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbce241",
   "metadata": {},
   "source": [
    "# Alter Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db3fea",
   "metadata": {},
   "source": [
    "## Add Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bec1ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "    ALTER TABLE hdfs_catalog.customers\n",
    "    ADD COLUMN phone_number STRING\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd68d7",
   "metadata": {},
   "source": [
    "## Rename Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers\n",
    "    RENAME COLUMN charges TO total_spent\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82e623",
   "metadata": {},
   "source": [
    "## Drop Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f0d0006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers\n",
    "    DROP COLUMN phone_number\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed42255",
   "metadata": {},
   "source": [
    "## Add Partition Field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers\n",
    "    ADD PARTITION FIELD bucket(16, customer_id)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4a180",
   "metadata": {},
   "source": [
    "## Create Branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62adcb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE hdfs_catalog.customers CREATE BRANCH dev_branch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce84cc1",
   "metadata": {},
   "source": [
    "## Create Branch by retaining snapshots for 30 days keeping at least the latest 3 snapshots, plus any snapshots created in the past 2 days at snapshot version 1234:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers CREATE BRANCH audit_branch \n",
    "    AS OF VERSION 1234 \n",
    "    RETAIN 30 DAYS \n",
    "    WITH SNAPSHOT RETENTION 3 SNAPSHOTS 2 DAYS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5daaa",
   "metadata": {},
   "source": [
    "## Create Tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "467bc1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers\n",
    "    CREATE TAG EOY_tag\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45457e96",
   "metadata": {},
   "source": [
    "## Create Tag at snapshot ID 1234, to retain a specific historical view of the table for analysis purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa289c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers CREATE TAG historical_tag \n",
    "    AS OF VERSION 1234\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ce92b",
   "metadata": {},
   "source": [
    "## Drop a Branch and Tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eff42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers DROP BRANCH dev_branch \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hdfs_catalog.customers DROP TAG EOY_tag  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cbad7",
   "metadata": {},
   "source": [
    "# Insert Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91905723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hdfs_catalog.customers VALUES\n",
    "        (1, 'John', 'Doe', 'john.doe@fakemail.co', 123.45, 'CA'),\n",
    "        (2, 'Jane', 'Smith', 'jane.smith@mockmail.org', 89.99, 'NY'),\n",
    "        (3, 'Alice', 'Johnson', 'alice.j@samplemail.net', 150.75, 'TX'),\n",
    "        (4, 'Bob', 'Brown', 'bob_brown@myemail.biz', 200.00, 'FL'),\n",
    "        (5, 'Eve', 'Davis', 'eve.davis@demoemail.com', 75.50, 'WA')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa85ce",
   "metadata": {},
   "source": [
    "# Merge Into/Upserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719ccef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure to create the `updates` table first:\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hdfs_catalog.updates (\n",
    "        customer_id INT,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        charges FLOAT,\n",
    "        state STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097a62f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Insert records into `updates`:\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hdfs_catalog.updates VALUES\n",
    "        (1, 'John', 'Doe', 'john.doe@fakemail.co', 130.00, 'CA'), \n",
    "        (6, 'Chris', 'Evans', 'chris.evans@hollywood.com', 300.00, 'CA'),\n",
    "        (7, 'Natasha', 'Romanoff', 'natasha.r@spyworld.com', 180.50, 'NY')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6269be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Upsert:\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO hdfs_catalog.customers AS target \n",
    "USING hdfs_catalog.updates AS source \n",
    "ON target.customer_id = source.customer_id \n",
    "WHEN MATCHED THEN \n",
    "  UPDATE SET * \n",
    "WHEN NOT MATCHED THEN \n",
    "  INSERT * \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a85f4",
   "metadata": {},
   "source": [
    "# Insert Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Overwrite Mode: Overwrites only the \"CA\" partition \n",
    "    \n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE hdfs_catalog.customers \n",
    "PARTITION (state = 'CA') \n",
    "SELECT customer_id, first_name, last_name, charges, email\n",
    "FROM hdfs_catalog.customers \n",
    "WHERE state = 'CA' \n",
    "GROUP BY customer_id \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Overwrite Mode: Overwrites all partitions with data in the query result \n",
    "    \n",
    "spark.sql(\"\"\"\n",
    "INSERT OVERWRITE hdfs_catalog.customers \n",
    "SELECT customer_id, first_name, last_name, email, charges \n",
    "FROM hdfs_catalog.customers \n",
    "WHERE state IN ('CA', 'NY') \n",
    "GROUP BY customer_id \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5e6a2",
   "metadata": {},
   "source": [
    "# Deletes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd638233",
   "metadata": {},
   "source": [
    "## Row-Level Delete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c927b0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 00:49:59 WARN S3InputStream: Unclosed input stream created by:\n",
      "\torg.apache.iceberg.aws.s3.S3InputStream.<init>(S3InputStream.java:74)\n",
      "\torg.apache.iceberg.aws.s3.S3InputFile.newStream(S3InputFile.java:85)\n",
      "\torg.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:100)\n",
      "\torg.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:76)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7$1.<init>(CloseableIterable.java:188)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7.iterator(CloseableIterable.java:187)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$0(CloseableIterable.java:109)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$1(CloseableIterable.java:136)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$1(CloseableIterable.java:136)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7$1.<init>(CloseableIterable.java:188)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7.iterator(CloseableIterable.java:187)\n",
      "\torg.apache.iceberg.ManifestGroup$1.iterator(ManifestGroup.java:347)\n",
      "\torg.apache.iceberg.ManifestGroup$1.iterator(ManifestGroup.java:305)\n",
      "\torg.apache.iceberg.util.ParallelIterable$ParallelIterator.lambda$new$1(ParallelIterable.java:69)\n",
      "\tjava.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tjava.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/01/09 00:49:59 WARN S3InputStream: Unclosed input stream created by:\n",
      "\torg.apache.iceberg.aws.s3.S3InputStream.<init>(S3InputStream.java:74)\n",
      "\torg.apache.iceberg.aws.s3.S3InputFile.newStream(S3InputFile.java:85)\n",
      "\torg.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:100)\n",
      "\torg.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:76)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7$1.<init>(CloseableIterable.java:188)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7.iterator(CloseableIterable.java:187)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$0(CloseableIterable.java:109)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$1(CloseableIterable.java:136)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable.lambda$filter$1(CloseableIterable.java:136)\n",
      "\torg.apache.iceberg.io.CloseableIterable$2.iterator(CloseableIterable.java:72)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7$1.<init>(CloseableIterable.java:188)\n",
      "\torg.apache.iceberg.io.CloseableIterable$7.iterator(CloseableIterable.java:187)\n",
      "\torg.apache.iceberg.ManifestGroup$1.iterator(ManifestGroup.java:347)\n",
      "\torg.apache.iceberg.ManifestGroup$1.iterator(ManifestGroup.java:305)\n",
      "\torg.apache.iceberg.util.ParallelIterable$ParallelIterator.lambda$new$1(ParallelIterable.java:69)\n",
      "\tjava.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tjava.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM hdfs_catalog.customers \n",
    "WHERE customer_id = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309d4ba",
   "metadata": {},
   "source": [
    "## Partition-Level Delete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "DELETE FROM hdfs_catalog.customers \n",
    "WHERE state = 'WA'  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58983f12",
   "metadata": {},
   "source": [
    "# Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b508c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "UPDATE hdfs_catalog.customers \n",
    "SET charges = charges * 1.1 \n",
    "WHERE state = 'CA'  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cda5e",
   "metadata": {},
   "source": [
    "# Read Query using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b658a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|          1|      John|      Doe|john.doe@fakemail.co| 123.45|   CA|\n",
      "|          5|       Eve|    Davis|eve.davis@demoema...|   75.5|   WA|\n",
      "|          2|      Jane|    Smith|jane.smith@mockma...|  89.99|   NY|\n",
      "|          3|     Alice|  Johnson|alice.j@samplemai...| 150.75|   TX|\n",
      "|          4|       Bob|    Brown|bob_brown@myemail...|  200.0|   FL|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hdfs_catalog.customers;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f7d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----+-------+\n",
      "|customer_id|first_name|last_name|state|charges|\n",
      "+-----------+----------+---------+-----+-------+\n",
      "|          1|      John|      Doe|   CA| 123.45|\n",
      "|          3|     Alice|  Johnson|   TX| 150.75|\n",
      "|          4|       Bob|    Brown|   FL|  200.0|\n",
      "+-----------+----------+---------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hdfs_catalog.high_value_customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b84aefd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|               email|charges|state|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "|          6|     Chris|    Evans|chris.evans@holly...|  330.0|   CA|\n",
      "|          5|       Eve|    Davis|eve.davis@demoema...|   75.5|   WA|\n",
      "|          2|      Jane|    Smith|jane.smith@mockma...|  89.99|   NY|\n",
      "|          3|     Alice|  Johnson|alice.j@samplemai...| 150.75|   TX|\n",
      "|          4|       Bob|    Brown|bob_brown@myemail...|  200.0|   FL|\n",
      "|          7|   Natasha| Romanoff|natasha.r@spyworl...|  180.5|   NY|\n",
      "+-----------+----------+---------+--------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hdfs_catalog.customers;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbdd40e",
   "metadata": {},
   "source": [
    "# Read Query using PySpark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50902867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"hadoop_catalog.customers\").filter(\"state = 'CA'\") \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1a952",
   "metadata": {},
   "source": [
    "# Time Travel Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a472191",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.customers TIMESTAMP AS OF '2024-10-26 15:30:00'\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.customers VERSION AS OF 12345678901234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e08cdbd",
   "metadata": {},
   "source": [
    "# Iceberg Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47342fca",
   "metadata": {},
   "source": [
    "## Expire Snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4536e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL hdfs_catalog.system.expire_snapshots(table => 'hdfs_catalog.customers', older_than => TIMESTAMP '2023-08-01 00:00:00', retain_last => 5 )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dc2d4",
   "metadata": {},
   "source": [
    "## Rollback to Snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CALL hdfs_catalog.system.rollback_to_snapshot(table => 'hdfs_catalog.customers', snapshot_id => 2711640443788239783 )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773108a5",
   "metadata": {},
   "source": [
    "## Remove Orphan Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry Run\n",
    "spark.sql(\"CALL hdfs_catalog.system.remove_orphan_files(table => 'hdfs_catalog.customers', dry_run => true)\")\n",
    "\n",
    "# Run Procedure\n",
    "spark.sql(\"CALL hdfs_catalog.system.remove_orphan_files(table => 'hdfs_catalog.customers',location => 's3a://diplakehouse/iceberg_book/customers/data' )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e66c9",
   "metadata": {},
   "source": [
    "## Rewrite Data Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "023ef635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\" CALL hdfs_catalog.system.rewrite_data_files(table => 'hdfs_catalog.customers', strategy => 'binpack')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6050e3",
   "metadata": {},
   "source": [
    "## Add Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CALL hdfs_catalog.system.add_files(table => 'hdfs_catalog.customers', path => 's3://my-bucket/new_data/')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
