{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b83342c",
   "metadata": {},
   "source": [
    "# Delta Lake Deep Dive: Hands-on with Apache Spark\n",
    "\n",
    "Executable, didactic walkthrough of Delta Lake operations using PySpark/Delta.\n",
    "\n",
    "**Sections**\n",
    "- Environment setup\n",
    "- Table creation (SQL + DataFrame API)\n",
    "- DML (INSERT/UPDATE/DELETE) + MERGE\n",
    "- Read queries + time travel\n",
    "- Delta management (HISTORY, VACUUM, OPTIMIZE where supported)\n",
    "- Schema evolution (ADD/RENAME/DROP)\n",
    "- Bounded (batch) vs Continuous (streaming) reads\n",
    "- Catalog configuration examples (reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000c277",
   "metadata": {},
   "source": [
    "## 0) Environment Setup\n",
    "\n",
    "If you’re running locally, you typically need:\n",
    "\n",
    "```bash\n",
    "pip install pyspark==3.4.1 delta-spark==2.4.0\n",
    "```\n",
    "\n",
    "This notebook assumes a Delta-enabled Spark environment (Databricks or OSS Delta via `delta-spark`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a1f21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.4.1\n",
      "BASE_DIR: /tmp/delta_demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use a predictable local path for path-based Delta examples\n",
    "BASE_DIR = os.environ.get('DELTA_DEMO_BASE', '/tmp/delta_demo')\n",
    "CUSTOMERS_PATH = os.path.join(BASE_DIR, 'customers_delta')\n",
    "STREAM_OUT_PATH = os.path.join(BASE_DIR, 'stream_out')\n",
    "STREAM_CHECKPOINT = os.path.join(BASE_DIR, 'stream_checkpoint')\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName('DeltaLakeDemo')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "print('PySpark version:', pyspark.__version__)\n",
    "print('BASE_DIR:', BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121ad86",
   "metadata": {},
   "source": [
    "### Output validation helpers\n",
    "A couple of tiny helpers so we can validate outputs after each major step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c4cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def assert_eq(actual, expected, msg=''):\n",
    "    if actual != expected:\n",
    "        raise AssertionError(f\"{msg} Expected {expected}, got {actual}\")\n",
    "    print(f\"✅ {msg} = {expected}\")\n",
    "\n",
    "def assert_true(cond, msg=''):\n",
    "    if not cond:\n",
    "        raise AssertionError(msg or 'Assertion failed')\n",
    "    print(f\"✅ {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a3d75",
   "metadata": {},
   "source": [
    "## 1) Create a Delta table (Spark SQL)\n",
    "\n",
    "We create a managed Delta table partitioned by `state` and then run basic DML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ce4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created table customers_delta_demo\n"
     ]
    }
   ],
   "source": [
    "# Clean slate\n",
    "spark.sql('DROP TABLE IF EXISTS customers_delta_demo')\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE customers_delta_demo (\n",
    "  customer_id INT,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  charges FLOAT,\n",
    "  state STRING\n",
    ") USING DELTA\n",
    "PARTITIONED BY (state)\n",
    "''')\n",
    "print('✅ Created table customers_delta_demo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2238f4a",
   "metadata": {},
   "source": [
    "### Insert + Update + Delete (SQL DML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f757839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DML completed\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "INSERT INTO customers_delta_demo VALUES\n",
    "  (10, 'Lin', 'Chan', 'lin.chan@example.com', 425.3, 'CA'),\n",
    "  (11, 'Iris', 'Huang', 'iris.huang@example.com', 820.0, 'NY')\n",
    "''')\n",
    "\n",
    "# Update charges for CA\n",
    "spark.sql(\"UPDATE customers_delta_demo SET charges = charges * 1.05 WHERE state = 'CA'\")\n",
    "\n",
    "# Delete low-charge customers\n",
    "spark.sql(\"DELETE FROM customers_delta_demo WHERE charges < 250\")\n",
    "print('✅ DML completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0018f7b",
   "metadata": {},
   "source": [
    "### Validate: row count and partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d71c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|email                 |charges|state|\n",
      "+-----------+----------+---------+----------------------+-------+-----+\n",
      "|11         |Iris      |Huang    |iris.huang@example.com|820.0  |NY   |\n",
      "|10         |Lin       |Chan     |lin.chan@example.com  |446.565|CA   |\n",
      "+-----------+----------+---------+----------------------+-------+-----+\n",
      "\n",
      "✅ customers_delta_demo row count = 2\n",
      "✅ distinct states are CA and NY\n"
     ]
    }
   ],
   "source": [
    "df = spark.table('customers_delta_demo')\n",
    "df.show(truncate=False)\n",
    "\n",
    "cnt = df.count()\n",
    "assert_eq(cnt, 2, 'customers_delta_demo row count')\n",
    "\n",
    "states = [r['state'] for r in df.select('state').distinct().collect()]\n",
    "assert_true(set(states) == {'CA', 'NY'}, 'distinct states are CA and NY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7616c05",
   "metadata": {},
   "source": [
    "## 2) Create/overwrite using the DataFrame API\n",
    "\n",
    "We overwrite the same table via the DataFrame API to demonstrate non-SQL writes.\n",
    "\n",
    "Note: This will replace the prior two-row contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af0b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overwrote customers_delta_demo via DataFrame API\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('email', StringType(), True),\n",
    "    StructField('charges', FloatType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 'John',  'Doe',   'john.doe@example.com',     250.5,  'CA'),\n",
    "    (2, 'Jane',  'Smith', 'jane.smith@example.com',  300.0,  'NY'),\n",
    "    (3, 'Alice', 'Brown', 'alice.brown@example.com', 180.75, 'TX'),\n",
    "]\n",
    "\n",
    "df_seed = spark.createDataFrame(data, schema=schema)\n",
    "df_seed.write.format('delta').mode('overwrite').partitionBy('state').saveAsTable('customers_delta_demo')\n",
    "print('✅ Overwrote customers_delta_demo via DataFrame API')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f28184",
   "metadata": {},
   "source": [
    "### Validate: the overwrite happened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3c0d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|email                  |charges|state|\n",
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "|1          |John      |Doe      |john.doe@example.com   |250.5  |CA   |\n",
      "|2          |Jane      |Smith    |jane.smith@example.com |300.0  |NY   |\n",
      "|3          |Alice     |Brown    |alice.brown@example.com|180.75 |TX   |\n",
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "\n",
      "✅ row count after overwrite = 3\n",
      "✅ TX row count = 1\n",
      "✅ minimum charges < 250 exists\n"
     ]
    }
   ],
   "source": [
    "df = spark.table('customers_delta_demo')\n",
    "df.orderBy('customer_id').show(truncate=False)\n",
    "assert_eq(df.count(), 3, 'row count after overwrite')\n",
    "\n",
    "tx_cnt = df.filter(F.col('state') == 'TX').count()\n",
    "assert_eq(tx_cnt, 1, 'TX row count')\n",
    "min_charges = df.agg(F.min('charges')).first()[0]\n",
    "assert_true(min_charges < 250, 'minimum charges < 250 exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efe66e",
   "metadata": {},
   "source": [
    "## 3) DML Operations (INSERT/DELETE/UPDATE) + MERGE\n",
    "\n",
    "We’ll add a few rows, delete some, update some, and then demonstrate an UPSERT with `MERGE INTO`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a75a1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ INSERT/DELETE/UPDATE complete\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO customers_delta_demo VALUES\n",
    "  (21, 'John',  'Doe',   'john.doe+21@example.com', 150.75, 'CA'),\n",
    "  (22, 'Alice', 'Smith', 'alice.smith@example.com', 200.50, 'NY'),\n",
    "  (23, 'Bob',   'Brown', 'bob.brown@example.com',   175.25, 'TX'),\n",
    "  (24, 'Emily', 'Davis', 'emily.davis@example.com',  220.30, 'FL')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DELETE FROM customers_delta_demo WHERE customer_id = 21\")\n",
    "spark.sql(\"DELETE FROM customers_delta_demo WHERE state = 'TX'\")\n",
    "spark.sql(\"UPDATE customers_delta_demo SET charges = charges * 1.1 WHERE state = 'CA'\")\n",
    "print('✅ INSERT/DELETE/UPDATE complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557403c",
   "metadata": {},
   "source": [
    "### Validate: deletions and updates applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9786063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ customer_id 21 deleted = 0\n",
      "✅ TX deleted = 0\n",
      "✅ CA rows exist\n",
      "✅ CA charges are positive\n",
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|email                  |charges|state|\n",
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "|1          |John      |Doe      |john.doe@example.com   |275.55 |CA   |\n",
      "|2          |Jane      |Smith    |jane.smith@example.com |300.0  |NY   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com|200.5  |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com|220.3  |FL   |\n",
      "+-----------+----------+---------+-----------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table('customers_delta_demo')\n",
    "assert_eq(df.filter(F.col('customer_id') == 21).count(), 0, 'customer_id 21 deleted')\n",
    "assert_eq(df.filter(F.col('state') == 'TX').count(), 0, 'TX deleted')\n",
    "ca_cnt = df.filter(F.col('state') == 'CA').count()\n",
    "assert_true(ca_cnt > 0, 'CA rows exist')\n",
    "assert_true(df.filter((F.col('state') == 'CA') & (F.col('charges') <= 0)).count() == 0, 'CA charges are positive')\n",
    "df.orderBy('customer_id').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7567ff5",
   "metadata": {},
   "source": [
    "### MERGE INTO (Upsert)\n",
    "\n",
    "We create a small staging table with updates and new inserts, then merge into `customers_delta_demo`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6cc2ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MERGE completed\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS staging_updates')\n",
    "spark.sql('''\n",
    "CREATE TABLE staging_updates (\n",
    "  customer_id INT,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  charges FLOAT,\n",
    "  state STRING\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "INSERT INTO staging_updates VALUES\n",
    "  (2,  'Jane', 'Smith', 'jane.smith+updated@example.com', 999.0, 'NY'),\n",
    "  (99, 'Zane', 'Otto',  'zane.otto@example.com',        1234.5, 'TX')\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "MERGE INTO customers_delta_demo AS target\n",
    "USING staging_updates AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "''')\n",
    "print('✅ MERGE completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3436f",
   "metadata": {},
   "source": [
    "### Validate: MERGE updated id=2 and inserted id=99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59866c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ customer_id=2 email updated\n",
      "✅ customer_id=2 charges updated to 999.0\n",
      "✅ customer_id=99 inserted = 1\n",
      "+-----------+----------+---------+------------------------------+-------+-----+\n",
      "|customer_id|first_name|last_name|email                         |charges|state|\n",
      "+-----------+----------+---------+------------------------------+-------+-----+\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55 |CA   |\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0  |NY   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5  |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3  |FL   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5 |TX   |\n",
      "+-----------+----------+---------+------------------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table('customers_delta_demo')\n",
    "r2 = df.filter(F.col('customer_id') == 2).select('email', 'charges').collect()[0]\n",
    "assert_true('updated' in r2['email'], 'customer_id=2 email updated')\n",
    "assert_true(abs(float(r2['charges']) - 999.0) < 1e-6, 'customer_id=2 charges updated to 999.0')\n",
    "assert_eq(df.filter(F.col('customer_id') == 99).count(), 1, 'customer_id=99 inserted')\n",
    "df.orderBy('customer_id').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ea6f3",
   "metadata": {},
   "source": [
    "## 4) Read Queries & Time Travel\n",
    "\n",
    "Delta tables keep transaction log history. We can time travel by version number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ff571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|8      |2025-12-02 19:56:10.165|null  |null    |MERGE                            |{predicate -> [\"(customer_id#8070 = customer_id#8076)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|null    |null     |7          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 3204, numTargetBytesRemoved -> 1580, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 653, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 383, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 267}|null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|7      |2025-12-02 19:56:08.162|null  |null    |UPDATE                           |{predicate -> [\"(state#5633 = CA)\"]}                                                                                                                                                           |null|null    |null     |6          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1551, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 124, scanTimeMs -> 61, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1551, rewriteTimeMs -> 63}                                                                                                                                                                                                                                                                                                         |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|6      |2025-12-02 19:56:07.746|null  |null    |DELETE                           |{predicate -> [\"(state#5034 = TX)\"]}                                                                                                                                                           |null|null    |null     |5          |Serializable  |false        |{numRemovedFiles -> 2, numRemovedBytes -> 3160, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 108, numDeletedRows -> 2, scanTimeMs -> 107, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                            |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|5      |2025-12-02 19:56:07.315|null  |null    |DELETE                           |{predicate -> [\"(customer_id#4176 = 21)\"]}                                                                                                                                                     |null|null    |null     |4          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1573, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 251, numDeletedRows -> 1, scanTimeMs -> 190, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 60}                                                                                                                                                                                                                                                                                                           |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|4      |2025-12-02 19:56:06.763|null  |null    |WRITE                            |{mode -> Append, partitionBy -> []}                                                                                                                                                            |null|null    |null     |3          |Serializable  |true         |{numFiles -> 4, numOutputRows -> 4, numOutputBytes -> 6327}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|3      |2025-12-02 19:56:05.896|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"state\"], properties -> {}}                                                                                                           |null|null    |null     |2          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 4725}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|2      |2025-12-02 19:56:04.219|null  |null    |UPDATE                           |{predicate -> [\"(state#1177 = CA)\"]}                                                                                                                                                           |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1551, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 360, scanTimeMs -> 191, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1551, rewriteTimeMs -> 168}                                                                                                                                                                                                                                                                                                       |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|1      |2025-12-02 19:56:03.386|null  |null    |WRITE                            |{mode -> Append, partitionBy -> []}                                                                                                                                                            |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 3131}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|0      |2025-12-02 19:56:00.98 |null  |null    |CREATE TABLE                     |{isManaged -> true, description -> null, partitionBy -> [\"state\"], properties -> {}}                                                                                                           |null|null    |null     |null       |Serializable  |true         |{}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "min_version: 0 max_version: 8\n",
      "✅ history has at least one version\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'customers_delta_demo')\n",
    "history_df = dt.history()\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "versions = [int(r['version']) for r in history_df.select('version').collect()]\n",
    "min_v, max_v = min(versions), max(versions)\n",
    "print('min_version:', min_v, 'max_version:', max_v)\n",
    "assert_true(max_v >= min_v, 'history has at least one version')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08238d7f",
   "metadata": {},
   "source": [
    "### Time travel read (versionAsOf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e774bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----+-------+-----+\n",
      "|customer_id|first_name|last_name|email|charges|state|\n",
      "+-----------+----------+---------+-----+-------+-----+\n",
      "+-----------+----------+---------+-----+-------+-----+\n",
      "\n",
      "✅ version 0 is empty (table created, no data yet) = 0\n"
     ]
    }
   ],
   "source": [
    "historic = spark.read.format('delta').option('versionAsOf', min_v).table('customers_delta_demo')\n",
    "historic.show(truncate=False)\n",
    "\n",
    "assert_eq(historic.count(), 0, \"version 0 is empty (table created, no data yet)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3923f22",
   "metadata": {},
   "source": [
    "## 5) Delta Management Commands\n",
    "\n",
    "- `VACUUM` cleans up unreferenced files (retention rules apply)\n",
    "- `OPTIMIZE` is environment-dependent\n",
    "- `DESCRIBE DETAIL/HISTORY` are useful diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4c5bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|8      |2025-12-02 19:56:10.165|null  |null    |MERGE                            |{predicate -> [\"(customer_id#8070 = customer_id#8076)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|null    |null     |7          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 3204, numTargetBytesRemoved -> 1580, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 653, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 383, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 267}|null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|7      |2025-12-02 19:56:08.162|null  |null    |UPDATE                           |{predicate -> [\"(state#5633 = CA)\"]}                                                                                                                                                           |null|null    |null     |6          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1551, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 124, scanTimeMs -> 61, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1551, rewriteTimeMs -> 63}                                                                                                                                                                                                                                                                                                         |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|6      |2025-12-02 19:56:07.746|null  |null    |DELETE                           |{predicate -> [\"(state#5034 = TX)\"]}                                                                                                                                                           |null|null    |null     |5          |Serializable  |false        |{numRemovedFiles -> 2, numRemovedBytes -> 3160, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 108, numDeletedRows -> 2, scanTimeMs -> 107, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                            |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|5      |2025-12-02 19:56:07.315|null  |null    |DELETE                           |{predicate -> [\"(customer_id#4176 = 21)\"]}                                                                                                                                                     |null|null    |null     |4          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1573, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 251, numDeletedRows -> 1, scanTimeMs -> 190, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 60}                                                                                                                                                                                                                                                                                                           |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|4      |2025-12-02 19:56:06.763|null  |null    |WRITE                            |{mode -> Append, partitionBy -> []}                                                                                                                                                            |null|null    |null     |3          |Serializable  |true         |{numFiles -> 4, numOutputRows -> 4, numOutputBytes -> 6327}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|3      |2025-12-02 19:56:05.896|null  |null    |CREATE OR REPLACE TABLE AS SELECT|{isManaged -> true, description -> null, partitionBy -> [\"state\"], properties -> {}}                                                                                                           |null|null    |null     |2          |Serializable  |false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 4725}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|2      |2025-12-02 19:56:04.219|null  |null    |UPDATE                           |{predicate -> [\"(state#1177 = CA)\"]}                                                                                                                                                           |null|null    |null     |1          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 1551, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 360, scanTimeMs -> 191, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1551, rewriteTimeMs -> 168}                                                                                                                                                                                                                                                                                                       |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|1      |2025-12-02 19:56:03.386|null  |null    |WRITE                            |{mode -> Append, partitionBy -> []}                                                                                                                                                            |null|null    |null     |0          |Serializable  |true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 3131}                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "|0      |2025-12-02 19:56:00.98 |null  |null    |CREATE TABLE                     |{isManaged -> true, description -> null, partitionBy -> [\"state\"], properties -> {}}                                                                                                           |null|null    |null     |null       |Serializable  |true         |{}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+------+------------------------------------+------------------------------------------+-----------+-----------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name                                      |description|location                                                   |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+------------------------------------------+-----------+-----------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |c6b5c57a-ccb3-404f-837d-af2c915f698d|spark_catalog.default.customers_delta_demo|null       |file:/home/jovyan/work/spark-warehouse/customers_delta_demo|2025-12-02 19:56:00.947|2025-12-02 19:56:10.165|[state]         |5       |7943       |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+------------------------------------------+-----------+-----------------------------------------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE HISTORY customers_delta_demo').show(truncate=False)\n",
    "try:\n",
    "    spark.sql('DESCRIBE DETAIL customers_delta_demo').show(truncate=False)\n",
    "except Exception as e:\n",
    "    print('DESCRIBE DETAIL not available in this environment:', type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719df69",
   "metadata": {},
   "source": [
    "### VACUUM (dry run + actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ebf082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|path|\n",
      "+----+\n",
      "+----+\n",
      "\n",
      "✅ VACUUM completed (retain 168 hours)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql('VACUUM customers_delta_demo DRY RUN').show(truncate=False)\n",
    "except Exception as e:\n",
    "    print('VACUUM DRY RUN not available:', type(e).__name__, str(e)[:200])\n",
    "\n",
    "try:\n",
    "    spark.sql('VACUUM customers_delta_demo RETAIN 168 HOURS')\n",
    "    print('✅ VACUUM completed (retain 168 hours)')\n",
    "except Exception as e:\n",
    "    print('VACUUM failed (environment-dependent):', type(e).__name__, str(e)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfb309",
   "metadata": {},
   "source": [
    "### OPTIMIZE (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0eddb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZE not supported in this environment — skipping.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql('OPTIMIZE customers_delta_demo ZORDER BY (state)')\n",
    "    print('✅ OPTIMIZE completed')\n",
    "except Exception:\n",
    "    print('OPTIMIZE not supported in this environment — skipping.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806ef34",
   "metadata": {},
   "source": [
    "## 6) More Spark SQL DDL/DML examples (Schema Evolution + CTAS)\n",
    "\n",
    "We’ll create a derived table via CTAS, then demonstrate ADD/RENAME/DROP columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc16b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created/updated high_value_customers\n",
      "+-----------+----------+---------+-----+-------+\n",
      "|customer_id|first_name|last_name|state|charges|\n",
      "+-----------+----------+---------+-----+-------+\n",
      "|2          |Jane      |Smith    |NY   |999.0  |\n",
      "|99         |Zane      |Otto     |TX   |1234.5 |\n",
      "+-----------+----------+---------+-----+-------+\n",
      "\n",
      "✅ high_value_customers columns correct\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE high_value_customers\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT customer_id, first_name, last_name, state, charges\n",
    "FROM customers_delta_demo\n",
    "WHERE charges > 500\n",
    "\"\"\")\n",
    "print('✅ Created/updated high_value_customers')\n",
    "\n",
    "hvc = spark.table('high_value_customers')\n",
    "hvc.show(truncate=False)\n",
    "assert_true(set(hvc.columns) == {'customer_id','first_name','last_name','state','charges'}, 'high_value_customers columns correct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542a48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ phone_number added\n",
      "✅ charges renamed to total_spent\n",
      "✅ phone_number dropped\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|customer_id|first_name|last_name|email                         |total_spent|state|\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0      |NY   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5      |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3      |FL   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5     |TX   |\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55     |CA   |\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# enable column mapping (and upgrade protocol if needed)\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE customers_delta_demo SET TBLPROPERTIES (\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.minReaderVersion' = '2',\n",
    "  'delta.minWriterVersion' = '5'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE customers_delta_demo ADD COLUMNS (phone_number STRING)\")\n",
    "cols = spark.table('customers_delta_demo').columns\n",
    "assert_true('phone_number' in cols, 'phone_number added')\n",
    "\n",
    "spark.sql(\"ALTER TABLE customers_delta_demo RENAME COLUMN charges TO total_spent\")\n",
    "cols = spark.table('customers_delta_demo').columns\n",
    "assert_true('total_spent' in cols and 'charges' not in cols, 'charges renamed to total_spent')\n",
    "\n",
    "spark.sql(\"ALTER TABLE customers_delta_demo DROP COLUMN phone_number\")\n",
    "cols = spark.table('customers_delta_demo').columns\n",
    "assert_true('phone_number' not in cols, 'phone_number dropped')\n",
    "\n",
    "spark.table('customers_delta_demo').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de056d",
   "metadata": {},
   "source": [
    "### Validate schema via DESCRIBE TABLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84a72035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|customer_id            |int      |null   |\n",
      "|first_name             |string   |null   |\n",
      "|last_name              |string   |null   |\n",
      "|email                  |string   |null   |\n",
      "|total_spent            |float    |null   |\n",
      "|state                  |string   |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|state                  |string   |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE TABLE customers_delta_demo').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cddab9",
   "metadata": {},
   "source": [
    "## 7) MERGE after rename (total_spent)\n",
    "\n",
    "After renaming, staging data must match the updated schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f1e65f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MERGE v2 completed\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS staging_updates_v2')\n",
    "spark.sql('''\n",
    "CREATE TABLE staging_updates_v2 (\n",
    "  customer_id INT,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  total_spent FLOAT,\n",
    "  state STRING\n",
    ") USING DELTA\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "INSERT INTO staging_updates_v2 VALUES\n",
    "  (10, 'Lin', 'Chan', 'lin.chan+v2@example.com', 1111.0, 'CA'),\n",
    "  (77, 'Nova', 'Kerr', 'nova.kerr@example.com',  2222.0, 'WA')\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "MERGE INTO customers_delta_demo AS target\n",
    "USING staging_updates_v2 AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "''')\n",
    "print('✅ MERGE v2 completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c80768",
   "metadata": {},
   "source": [
    "### Validate: id=10 updated and id=77 inserted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36a890a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ customer_id=10 email updated (v2)\n",
      "✅ customer_id=10 total_spent updated to 1111.0\n",
      "✅ customer_id=77 inserted = 1\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|customer_id|first_name|last_name|email                         |total_spent|state|\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55     |CA   |\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0      |NY   |\n",
      "|10         |Lin       |Chan     |lin.chan+v2@example.com       |1111.0     |CA   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5      |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3      |FL   |\n",
      "|77         |Nova      |Kerr     |nova.kerr@example.com         |2222.0     |WA   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5     |TX   |\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table('customers_delta_demo')\n",
    "r10 = df.filter(F.col('customer_id') == 10).select('email', 'total_spent').collect()[0]\n",
    "assert_true('v2' in r10['email'], 'customer_id=10 email updated (v2)')\n",
    "assert_true(abs(float(r10['total_spent']) - 1111.0) < 1e-6, 'customer_id=10 total_spent updated to 1111.0')\n",
    "assert_eq(df.filter(F.col('customer_id') == 77).count(), 1, 'customer_id=77 inserted')\n",
    "df.orderBy('customer_id').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d572d06",
   "metadata": {},
   "source": [
    "## 8) Path-based Delta table examples (bounded reads)\n",
    "\n",
    "Write a path-based Delta table, then read it back (latest + versionAsOf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4c3cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote path-based Delta table to: /tmp/delta_demo/customers_delta\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|customer_id|first_name|last_name|email                         |total_spent|state|\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0      |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3      |FL   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5      |NY   |\n",
      "|10         |Lin       |Chan     |lin.chan+v2@example.com       |1111.0     |CA   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5     |TX   |\n",
      "|77         |Nova      |Kerr     |nova.kerr@example.com         |2222.0     |WA   |\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55     |CA   |\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "\n",
      "✅ path-based latest read has rows\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(CUSTOMERS_PATH):\n",
    "    shutil.rmtree(CUSTOMERS_PATH)\n",
    "\n",
    "spark.table('customers_delta_demo').write.format('delta').mode('overwrite').save(CUSTOMERS_PATH)\n",
    "print('✅ Wrote path-based Delta table to:', CUSTOMERS_PATH)\n",
    "\n",
    "latest_path_df = spark.read.format('delta').load(CUSTOMERS_PATH)\n",
    "latest_path_df.show(truncate=False)\n",
    "assert_true(latest_path_df.count() > 0, 'path-based latest read has rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b907bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                            |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-12-02 19:56:25.839|null  |null    |WRITE    |{mode -> Overwrite, partitionBy -> []}|null|null    |null     |null       |Serializable  |false        |{numFiles -> 7, numOutputRows -> 7, numOutputBytes -> 12718}|null        |Apache-Spark/3.4.1 Delta-Lake/2.4.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|customer_id|first_name|last_name|email                         |total_spent|state|\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0      |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3      |FL   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5      |NY   |\n",
      "|10         |Lin       |Chan     |lin.chan+v2@example.com       |1111.0     |CA   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5     |TX   |\n",
      "|77         |Nova      |Kerr     |nova.kerr@example.com         |2222.0     |WA   |\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55     |CA   |\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "\n",
      "✅ path-based historic read has rows\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "dt_path = DeltaTable.forPath(spark, CUSTOMERS_PATH)\n",
    "hist = dt_path.history()\n",
    "hist.show(truncate=False)\n",
    "versions = [int(r['version']) for r in hist.select('version').collect()]\n",
    "min_v = min(versions)\n",
    "\n",
    "path_historic = spark.read.format('delta').option('versionAsOf', min_v).load(CUSTOMERS_PATH)\n",
    "path_historic.show(truncate=False)\n",
    "assert_true(path_historic.count() > 0, 'path-based historic read has rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ff26b",
   "metadata": {},
   "source": [
    "## 9) Continuous (streaming) reads in PySpark\n",
    "\n",
    "Runnable streaming template using `trigger(once=True)` so it runs one micro-batch and stops.\n",
    "We stream from the *path-based* Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f491e688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming micro-batch (once) completed\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "for p in [STREAM_OUT_PATH, STREAM_CHECKPOINT]:\n",
    "    if os.path.exists(p):\n",
    "        shutil.rmtree(p)\n",
    "\n",
    "stream_df = spark.readStream.format('delta').load(CUSTOMERS_PATH)\n",
    "\n",
    "q = (\n",
    "    stream_df\n",
    "    .groupBy('state')\n",
    "    .count()\n",
    "    .writeStream\n",
    "    .outputMode('complete')\n",
    "    .format('console')\n",
    "    .option('checkpointLocation', STREAM_CHECKPOINT)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "q.awaitTermination()\n",
    "print('✅ Streaming micro-batch (once) completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aed5a",
   "metadata": {},
   "source": [
    "### Append data and run streaming again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8219174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Appended new rows to path-based table\n",
      "✅ Second streaming micro-batch completed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row, functions as F\n",
    "\n",
    "# Load existing schema from the delta path\n",
    "base = spark.read.format(\"delta\").load(CUSTOMERS_PATH)\n",
    "\n",
    "new_rows = [\n",
    "    Row(customer_id=500, first_name=\"Stream\", last_name=\"One\", email=\"stream.one@example.com\", total_spent=10.0, state=\"CA\"),\n",
    "    Row(customer_id=501, first_name=\"Stream\", last_name=\"Two\", email=\"stream.two@example.com\", total_spent=20.0, state=\"WA\"),\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "# Force types to match existing table\n",
    "new_df = (\n",
    "    new_df\n",
    "    .withColumn(\"customer_id\", F.col(\"customer_id\").cast(base.schema[\"customer_id\"].dataType))\n",
    "    .withColumn(\"total_spent\", F.col(\"total_spent\").cast(base.schema[\"total_spent\"].dataType))\n",
    ")\n",
    "\n",
    "# (optional but nice) reorder columns to match\n",
    "new_df = new_df.select([c.name for c in base.schema.fields])\n",
    "\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(CUSTOMERS_PATH)\n",
    "print('✅ Appended new rows to path-based table')\n",
    "\n",
    "q2 = (\n",
    "    spark.readStream.format('delta').load(CUSTOMERS_PATH)\n",
    "    .groupBy('state')\n",
    "    .count()\n",
    "    .writeStream\n",
    "    .outputMode('complete')\n",
    "    .format('console')\n",
    "    .option('checkpointLocation', STREAM_CHECKPOINT)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "q2.awaitTermination()\n",
    "print('✅ Second streaming micro-batch completed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a74b9d",
   "metadata": {},
   "source": [
    "### Validate: appended rows exist in batch read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65866049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ stream-appended rows present = 2\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|customer_id|first_name|last_name|email                         |total_spent|state|\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "|1          |John      |Doe      |john.doe@example.com          |275.55     |CA   |\n",
      "|2          |Jane      |Smith    |jane.smith+updated@example.com|999.0      |NY   |\n",
      "|10         |Lin       |Chan     |lin.chan+v2@example.com       |1111.0     |CA   |\n",
      "|22         |Alice     |Smith    |alice.smith@example.com       |200.5      |NY   |\n",
      "|24         |Emily     |Davis    |emily.davis@example.com       |220.3      |FL   |\n",
      "|77         |Nova      |Kerr     |nova.kerr@example.com         |2222.0     |WA   |\n",
      "|99         |Zane      |Otto     |zane.otto@example.com         |1234.5     |TX   |\n",
      "|500        |Stream    |One      |stream.one@example.com        |10.0       |CA   |\n",
      "|501        |Stream    |Two      |stream.two@example.com        |20.0       |WA   |\n",
      "+-----------+----------+---------+------------------------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest = spark.read.format('delta').load(CUSTOMERS_PATH)\n",
    "assert_eq(latest.filter(F.col('customer_id').isin([500, 501])).count(), 2, 'stream-appended rows present')\n",
    "latest.orderBy('customer_id').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e9b0b",
   "metadata": {},
   "source": [
    "## 10) Catalog configurations in Spark (reference)\n",
    "\n",
    "### Hadoop catalog\n",
    "```text\n",
    "spark.sql.catalog.hadoop_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
    "spark.sql.catalog.hadoop_catalog.type=hadoop\n",
    "spark.sql.catalog.hadoop_catalog.warehouse=s3a://deltalake/warehouse\n",
    "```\n",
    "\n",
    "### Hive Metastore catalog\n",
    "```text\n",
    "spark.sql.catalog.hive_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
    "spark.sql.catalog.hive_catalog.type=hive\n",
    "spark.sql.catalog.hive_catalog.uri=thrift://metastore-host:9083\n",
    "```\n",
    "\n",
    "### AWS Glue Data Catalog\n",
    "```text\n",
    "spark.sql.catalog.glue_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
    "spark.sql.catalog.glue_catalog.type=glue\n",
    "spark.sql.catalog.glue_catalog.warehouse=s3a://deltalake/warehouse\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298673c1",
   "metadata": {},
   "source": [
    "## Closing Notes\n",
    "\n",
    "- This notebook is runnable end-to-end, with validation checks after key operations.\n",
    "- `OPTIMIZE` support varies by environment; the notebook auto-skips if not supported.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
