{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Deep Dive - Spark Hands-on Notebook\n",
    "\n",
    "This notebook accompanies Chapter 5 and demonstrates how to use Delta Lake with Apache Spark. Please run each code block in sequence for a step-by-step exploration of Delta Lake features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "- **Recommended**: Use the provided Docker Compose environment for a reproducible setup.\n",
    "- Or, use a Spark 3.x and Delta Lake 2.x+ local install with:\n",
    "\n",
    "```bash\n",
    "pip install pyspark==3.4.1 delta-spark==2.4.0 findspark\n",
    "```\n",
    "- Java 8+ is required for Spark.\n",
    "- The remainder of this notebook will run inside a suitable Jupyter or Databricks environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Check Spark & Delta import\n",
    "import pyspark\n",
    "from delta import *\n",
    "print(\"PySpark version:\", pyspark.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Delta Table Creation (SQL & DataFrame)\n",
    "- Inserts & Queries\n",
    "- Update, Delete, Merge (Upsert)\n",
    "- Schema Evolution\n",
    "- Time Travel\n",
    "- Change Data Feed (CDC)\n",
    "- Optimization (Z-Ordering, Vacuum)\n",
    "- Streaming Reads/Writes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Delta Table Creation with Spark\n",
    "\n",
    "Let's start by creating a Delta table using both SQL and the DataFrame API. We'll use a local file path for simplicity. Ensure the working directory has the right permissions, or update the path as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName('DeltaLakeDemo')\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print('Spark session started!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Delta table delta_test!\n"
     ]
    }
   ],
   "source": [
    "# Create a Delta table using SQL DDL\n",
    "spark.sql('DROP TABLE IF EXISTS delta_test')\n",
    "spark.sql('''\n",
    "    CREATE TABLE delta_test (\n",
    "        id INT, name STRING, amount FLOAT\n",
    "    ) USING DELTA\n",
    "''')\n",
    "print('Created Delta table delta_test!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created at /tmp/delta_table_df\n"
     ]
    }
   ],
   "source": [
    "# Create a Delta table using the DataFrame API\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('amount', FloatType(), True)\n",
    "])\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.write.format('delta').mode('overwrite').save('/tmp/delta_table_df')\n",
    "print('Delta table created at /tmp/delta_table_df')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Insert Records and Basic Queries\n",
    "\n",
    "Let's insert records into our Delta tables and show how to query the data using both SQL and the DataFrame API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 records inserted into delta_test!\n",
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  1|Alice| 120.5|\n",
      "|  2|  Bob|  85.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert records using SQL\n",
    "spark.sql('''\n",
    "    INSERT INTO delta_test VALUES\n",
    "        (1, 'Alice', 120.5),\n",
    "        (2, 'Bob', 85.0),\n",
    "        (3, 'Carol', 310.8)\n",
    "''')\n",
    "print('3 records inserted into delta_test!')\n",
    "\n",
    "# Query records using SQL\n",
    "spark.sql('SELECT * FROM delta_test').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 2 records to /tmp/delta_table_df\n",
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  4|David| 220.1|\n",
      "|  5|  Eve|  99.5|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert records using DataFrame API (append to /tmp/delta_table_df)\n",
    "data = [\n",
    "    (4, 'David', 220.1),\n",
    "    (5, 'Eve', 99.5)\n",
    "]\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.write.format('delta').mode('append').save('/tmp/delta_table_df')\n",
    "print('Appended 2 records to /tmp/delta_table_df')\n",
    "\n",
    "# Query DataFrame-based Delta table\n",
    "df_read = spark.read.format('delta').load('/tmp/delta_table_df')\n",
    "df_read.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You should now see your inserted Delta Lake records both via SQL and DataFrame API. Next, we'll explore updates, deletes, and upserts (MERGE INTO) with Delta tables.\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update, Delete, and Upsert (MERGE) Operations\n",
    "\n",
    "Delta Lake supports powerful transactional DML statements. Let's perform row-level `UPDATE`, `DELETE`, and upsert with `MERGE INTO`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  1|Alice| 120.5|\n",
      "|  2|  Bob| 105.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update a row by SQL\n",
    "spark.sql(\"UPDATE delta_test SET amount = amount + 20 WHERE id = 2\")\n",
    "spark.sql(\"SELECT * FROM delta_test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  2|  Bob| 105.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete a row by SQL\n",
    "spark.sql(\"DELETE FROM delta_test WHERE id = 1\")\n",
    "spark.sql(\"SELECT * FROM delta_test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  2|  Bob| 200.0|\n",
      "|  4|Daisy| 177.7|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Upsert rows using MERGE INTO\n",
    "# First, add a table with updates\n",
    "spark.sql(\"DROP TABLE IF EXISTS delta_updates\")\n",
    "spark.sql('''\n",
    "    CREATE TABLE delta_updates (\n",
    "        id INT, name STRING, amount FLOAT\n",
    "    ) USING DELTA\n",
    "''')\n",
    "spark.sql('''\n",
    "    INSERT INTO delta_updates VALUES\n",
    "        (2, 'Bob', 200.0),   -- UPDATE: id=2 will be updated\n",
    "        (4, 'Daisy', 177.7)  -- INSERT: id=4 is new\n",
    "''')\n",
    "\n",
    "spark.sql('''\n",
    "MERGE INTO delta_test t\n",
    "USING delta_updates u\n",
    "ON t.id = u.id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "''')\n",
    "spark.sql('SELECT * FROM delta_test').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schema Evolution: Add, Drop, Rename Columns\n",
    "\n",
    "Delta Lake supports schema evolution, meaning you can add, drop, and rename columns on Delta tables easily. Let's walk through the major operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   null|\n",
      "|    name|   string|   null|\n",
      "|  amount|    float|   null|\n",
      "|   email|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column\n",
    "spark.sql(\"ALTER TABLE delta_test ADD COLUMNS (email STRING)\")\n",
    "spark.sql(\"DESCRIBE TABLE delta_test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upgrade the table to support drop/rename columns (enable column mapping by name)\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE delta_test SET TBLPROPERTIES (\n",
    "    'delta.columnMapping.mode' = 'name',\n",
    "    'delta.minReaderVersion' = '2',\n",
    "    'delta.minWriterVersion' = '5'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   null|\n",
      "|    name|   string|   null|\n",
      "|  amount|    float|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop a column\n",
    "spark.sql(\"ALTER TABLE delta_test DROP COLUMN email\")\n",
    "spark.sql(\"DESCRIBE TABLE delta_test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|           id|      int|   null|\n",
      "|customer_name|   string|   null|\n",
      "|       amount|    float|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column (Delta Lake >= 2.0.0)\n",
    "spark.sql(\"ALTER TABLE delta_test RENAME COLUMN name TO customer_name\")\n",
    "spark.sql(\"DESCRIBE TABLE delta_test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No column 'name' to rename.\n",
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|           id|      int|   null|\n",
      "|customer_name|   string|   null|\n",
      "|       amount|    float|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Idempotent rename: 'name' -> 'customer_name' only if 'name' exists\n",
    "cols = [f.name for f in spark.table('delta_test').schema.fields]\n",
    "if 'name' in cols:\n",
    "    spark.sql(\"ALTER TABLE delta_test RENAME COLUMN name TO customer_name\")\n",
    "else:\n",
    "    print(\"No column 'name' to rename.\")\n",
    "spark.sql(\"DESCRIBE TABLE delta_test\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ... do other schema evolution steps or demo updates here if needed ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   null|\n",
      "|    name|   string|   null|\n",
      "|  amount|    float|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Idempotent rename: 'customer_name' -> 'name' only if 'customer_name' exists\n",
    "cols = [f.name for f in spark.table('delta_test').schema.fields]\n",
    "if 'customer_name' in cols:\n",
    "    spark.sql(\"ALTER TABLE delta_test RENAME COLUMN customer_name TO name\")\n",
    "else:\n",
    "    print(\"No column 'customer_name' to rename.\")\n",
    "spark.sql(\"DESCRIBE TABLE delta_test\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "That's schema evolution! Add, drop, or rename columns safely—even after table creation. Next, let's see how Time Travel works in Delta Lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Travel (Querying Past Table Versions)\n",
    "\n",
    "Delta Lake lets you query your table as it appeared at a previous version or timestamp! This enables rollback, auditing, and recovery after accidental changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  2|  Bob| 200.0|\n",
      "|  4|Daisy| 177.7|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show current table state\n",
    "delta_df = spark.read.format('delta').table('delta_test')\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress FutureWarning about Pandas datetime64 dtype for cleaner output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Lake Version History\n",
    "\n",
    "Below, we show the Delta Lake table's commit history. Note that version 0 is just the table creation: it won't have any data! Use a version **after the first INSERT** for meaningful time travel queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+-----------------+\n",
      "|version|timestamp              |operation        |\n",
      "+-------+-----------------------+-----------------+\n",
      "|9      |2025-12-02 19:57:03.735|RENAME COLUMN    |\n",
      "|8      |2025-12-02 19:57:03.43 |RENAME COLUMN    |\n",
      "|7      |2025-12-02 19:57:03.145|DROP COLUMNS     |\n",
      "|6      |2025-12-02 19:57:02.861|SET TBLPROPERTIES|\n",
      "|5      |2025-12-02 19:57:02.553|ADD COLUMNS      |\n",
      "|4      |2025-12-02 19:57:02.112|MERGE            |\n",
      "|3      |2025-12-02 19:57:00.484|DELETE           |\n",
      "|2      |2025-12-02 19:56:59.803|UPDATE           |\n",
      "|1      |2025-12-02 19:56:58.273|WRITE            |\n",
      "|0      |2025-12-02 19:56:54.91 |CREATE TABLE     |\n",
      "+-------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show commit/version history to find which versions have your data\n",
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'delta_test')\n",
    "dt.history().select('version', 'timestamp', 'operation').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel Demo (Querying Past Table Versions)\n",
    "\n",
    "Set `version_num` to a version after the CREATE TABLE (usually version 1 or later) to view data as it was after the first insert/update/merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|        operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      9|2025-12-02 19:57:...|  null|    null|    RENAME COLUMN|{oldColumnPath ->...|null|    null|     null|          8|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "|      8|2025-12-02 19:57:...|  null|    null|    RENAME COLUMN|{oldColumnPath ->...|null|    null|     null|          7|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "|      7|2025-12-02 19:57:...|  null|    null|     DROP COLUMNS|{columns -> [\"ema...|null|    null|     null|          6|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "|      6|2025-12-02 19:57:...|  null|    null|SET TBLPROPERTIES|{properties -> {\"...|null|    null|     null|          5|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "|      5|2025-12-02 19:57:...|  null|    null|      ADD COLUMNS|{columns -> [{\"co...|null|    null|     null|          4|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "|      4|2025-12-02 19:57:...|  null|    null|            MERGE|{predicate -> [\"(...|null|    null|     null|          3|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.4....|\n",
      "|      3|2025-12-02 19:57:...|  null|    null|           DELETE|{predicate -> [\"(...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      2|2025-12-02 19:56:...|  null|    null|           UPDATE|{predicate -> [\"(...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.4....|\n",
      "|      1|2025-12-02 19:56:...|  null|    null|            WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.4....|\n",
      "|      0|2025-12-02 19:56:...|  null|    null|     CREATE TABLE|{isManaged -> tru...|null|    null|     null|       null|  Serializable|         true|                  {}|        null|Apache-Spark/3.4....|\n",
      "+-------+--------------------+------+--------+-----------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the current version\n",
    "import delta\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forName(spark, 'delta_test')\n",
    "history_df = dt.history()\n",
    "history_df.show()  # Shows operation history, version, and timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|amount|\n",
      "+---+----+------+\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time travel best practice: Always inspect schema before queries!\n",
    "old_df = spark.read.format('delta').option('versionAsOf', 0).table('delta_test')\n",
    "# See which columns exist (use them when querying old_df)\n",
    "old_df.printSchema()\n",
    "# Now query with the actual columns at that version ('name', not 'customer_name')\n",
    "old_df.select('id','name','amount').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema at Delta version 1 : ['id', 'name', 'amount']\n",
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  1|Alice| 120.5|\n",
      "|  2|  Bob|  85.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# General pattern for time travel: inspect schema, then select existing columns\n",
    "version_num = 1  # Set this to the version you want\n",
    "tt_df = spark.read.format('delta').option('versionAsOf', version_num).table('delta_test')\n",
    "print('Schema at Delta version', version_num, ':', tt_df.columns)\n",
    "tt_df.show()  # Will always work, shows all columns that exist at that version\n",
    "\n",
    "# Optionally, select specific columns as reported by printSchema()/columns\n",
    "# Example: tt_df.select('id', 'customer_name', 'amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|  3|Carol| 310.8|\n",
      "|  1|Alice| 120.5|\n",
      "|  2|  Bob| 105.0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query an older version of the Delta table (pick a previous version like 0 or 1)\n",
    "old_df = spark.read.format('delta').option('versionAsOf', 2).table('delta_test')\n",
    "old_df.show()\n",
    "\n",
    "# Or query as of a timestamp (replace with suitable timestamp from history_df)\n",
    "# old_df = spark.read.format('delta').option('timestampAsOf', '2024-01-01 00:00:00').table('delta_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change Data Feed (CDC)\n",
    "\n",
    "Delta Lake supports Change Data Feed, letting you query only changed (insert/update/delete) rows between versions, if enabled (Delta Lake >= 2.0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC not available between chosen versions or not supported:  Error getting change data for range [1 , 3] as change data was not\n",
      "recorded for version [1]. If you've enabled change data feed on this table,\n",
      "use `DESCRIBE HISTORY` to see when it was first enabled.\n",
      "Otherwise, to start recording change data, use `ALTER TABLE table_name SET TBLPROPERTIES\n",
      "(delta.enableChangeDataFeed=true)`.\n"
     ]
    }
   ],
   "source": [
    "# Enable CDC feature on an existing table (newer Delta Lake only)\n",
    "spark.sql(\"ALTER TABLE delta_test SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "# Get CDF between two versions. Pick versions that include some change events!\n",
    "try:\n",
    "    # You can use dt.history().show() to pick meaningful version ranges\n",
    "    cdf = spark.read.format(\"delta\") \\\n",
    "        .option(\"readChangeData\", \"true\") \\\n",
    "        .option(\"startingVersion\", 1) \\\n",
    "        .option(\"endingVersion\", 3) \\\n",
    "        .table(\"delta_test\")\n",
    "    cdf.show()\n",
    "except Exception as e:\n",
    "    print(\"CDC not available between chosen versions or not supported: \", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If CDF is not supported by your Delta version, this cell will error — just comment it out or upgrade Delta Lake as needed.\n",
    "\n",
    "Now, let's optimize the Delta table using Z-Order and VACUUM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC enabled at version 10. We'll query after this version.\n",
      "CDC enabled, but no further versions with changes to display yet.\n"
     ]
    }
   ],
   "source": [
    "# Robust CDC demo - only show if enabled and with real changes\n",
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "dt = DeltaTable.forName(spark, 'delta_test')\n",
    "hist_pd = dt.history().toPandas()\n",
    "cdc_enabled_versions = hist_pd[\n",
    "    hist_pd['operationParameters'].astype(str).str.contains('delta.enableChangeDataFeed', na=False)\n",
    "]['version']\n",
    "\n",
    "if not cdc_enabled_versions.empty:\n",
    "    cdc_start = int(cdc_enabled_versions.values[0])\n",
    "    print(f\"CDC enabled at version {cdc_start}. We'll query after this version.\")\n",
    "    try:\n",
    "        latest_ver = int(hist_pd['version'].max())\n",
    "        if latest_ver > cdc_start:\n",
    "            cdf = spark.read.format(\"delta\") \\\n",
    "                .option(\"readChangeData\", \"true\") \\\n",
    "                .option(\"startingVersion\", cdc_start) \\\n",
    "                .option(\"endingVersion\", latest_ver) \\\n",
    "                .table(\"delta_test\")\n",
    "            if cdf.count() == 0:\n",
    "                print(\"CDC enabled, but no changes recorded between these versions.\")\n",
    "            else:\n",
    "                cdf.show()\n",
    "        else:\n",
    "            print(\"CDC enabled, but no further versions with changes to display yet.\")\n",
    "    except Exception as e:\n",
    "        print(\"CDC error: \", e)\n",
    "else:\n",
    "    print(\"CDC was never enabled for this table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC enabled at version 10, now querying changes up to 10\n",
      "+---+----+------+------------+---------------+-----------------+\n",
      "| id|name|amount|_change_type|_commit_version|_commit_timestamp|\n",
      "+---+----+------+------------+---------------+-----------------+\n",
      "+---+----+------+------------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate CDC with real data: perform an operation after CDC is enabled\n",
    "# Insert, update, or delete after enabling CDC so changes appear in the feed\n",
    "spark.sql(\"INSERT INTO delta_test VALUES (200, 'AfterCDC', 500.0)\")\n",
    "spark.sql(\"UPDATE delta_test SET amount = amount + 999 WHERE id = 2\")\n",
    "spark.sql(\"DELETE FROM delta_test WHERE id = 3\")\n",
    "\n",
    "# Now rerun CDC (between CDC enable version and latest)\n",
    "cdc_enabled_versions = hist_pd[\n",
    "    hist_pd['operationParameters'].astype(str).str.contains('delta.enableChangeDataFeed', na=False)\n",
    "]['version']\n",
    "if not cdc_enabled_versions.empty:\n",
    "    cdc_start = int(cdc_enabled_versions.values[0])\n",
    "    latest_ver = int(hist_pd['version'].max())\n",
    "    print(f\"CDC enabled at version {cdc_start}, now querying changes up to {latest_ver}\")\n",
    "    try:\n",
    "        cdf = spark.read.format(\"delta\") \\\n",
    "            .option(\"readChangeData\", \"true\") \\\n",
    "            .option(\"startingVersion\", cdc_start) \\\n",
    "            .option(\"endingVersion\", latest_ver) \\\n",
    "            .table(\"delta_test\")\n",
    "        cdf.show()\n",
    "    except Exception as e:\n",
    "        print(\"CDC error after data changes: \", e)\n",
    "else:\n",
    "    print(\"CDC was never enabled for this table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization: Z-Order, VACUUM, and Table Maintenance\n",
    "\n",
    "Delta Lake supports table optimization for query performance and storage cleanup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZE not supported in this Spark. Skipping: Z-Ordering column customer_name does not exist in data schema.\n"
     ]
    }
   ],
   "source": [
    "# Optimize table data layout (requires Databricks or Delta OSS 2.0+ SQL)\n",
    "# This command may not be available in open source Spark unless you use native OPTIMIZE extension.\n",
    "try:\n",
    "    spark.sql(\"OPTIMIZE delta_test ZORDER BY (customer_name)\")\n",
    "except Exception as e:\n",
    "    print(f'OPTIMIZE not supported in this Spark. Skipping: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: OPTIMIZE delta_test ZORDER BY (name)\n"
     ]
    }
   ],
   "source": [
    "# Robust Z-Order OPTIMIZE: only run for columns that exist and if supported\n",
    "cols = [f.name for f in spark.table('delta_test').schema.fields]\n",
    "col_to_zorder = 'name' if 'name' in cols else cols[0]  # Use name if present, else first column\n",
    "try:\n",
    "    sql = f\"OPTIMIZE delta_test ZORDER BY ({col_to_zorder})\"\n",
    "    print(f\"Running: {sql}\")\n",
    "    spark.sql(sql)\n",
    "except Exception as e:\n",
    "    print(f'OPTIMIZE not supported in this Spark or on this data. Skipping: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove old files no longer needed for time travel\n",
    "spark.sql(\"VACUUM delta_test RETAIN 168 HOURS\")  # retains files for 7 days by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "VACUUM reclaims disk from unreferenced files (check your data retention policy before running it in production!).\n",
    "\n",
    "Next: Streaming reads and writes with Delta Lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Reads and Writes\n",
    "\n",
    "Delta Lake supports both batch (bounded) and streaming (continuous) data pipelines directly on tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|amount|\n",
      "+---+-----+------+\n",
      "|100|Fresh|  88.8|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up a directory for file-based streaming demo\n",
    "import os, shutil\n",
    "stream_dir = \"/tmp/delta_stream_source\"\n",
    "if os.path.exists(stream_dir):\n",
    "    shutil.rmtree(stream_dir)\n",
    "os.makedirs(stream_dir)\n",
    "\n",
    "# Define the input schema to match written files\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "input_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),  # use 'customer_name' if your schema is that\n",
    "    StructField(\"amount\", FloatType()),\n",
    "])\n",
    "\n",
    "# Start streaming read from that directory\n",
    "streaming_input_df = spark.readStream.schema(input_schema).json(stream_dir)\n",
    "\n",
    "# Start streaming write to Delta Lake table\n",
    "query = streaming_input_df.writeStream.format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/delta_test_checkpoint\") \\\n",
    "    .start(\"/tmp/delta_table_stream\")\n",
    "\n",
    "# Write a new file to the streaming source (simulates real-time event)\n",
    "import json\n",
    "data = {\"id\": 100, \"name\": \"Fresh\", \"amount\": 88.8}\n",
    "with open(os.path.join(stream_dir, \"test-event.json\"), \"w\") as f:\n",
    "    f.write(json.dumps(data))\n",
    "\n",
    "import time\n",
    "time.sleep(10)  # Let streaming job pick up the file and commit it\n",
    "query.stop()\n",
    "\n",
    "# Query the Delta table to see the new row\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta_table_stream\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from a Delta table as a stream\n",
    "read_query = spark.readStream.format(\"delta\").load(\"/tmp/delta_table_stream\")\n",
    "# To output the stream in the notebook, you would need to write it to a sink (console/parquet, etc.) in production.\n",
    "# Here, we'll just show the DataFrame definition.\n",
    "read_query.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "That concludes the hands-on deep dive in Delta Lake with Spark!\n",
    "\n",
    "- CRUD, schema evolution, DML/transactions, time travel, CDC, optimization, and streaming.\n",
    "\n",
    "For more, refer to Delta Lake documentation or try more complex scenarios with your own datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
